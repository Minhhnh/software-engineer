# Tenstorrent LLM Inference – Technical Deep Dive

Tenstorrent is an AI hardware startup building next-generation neural processors, with a strong focus on large language model (LLM) inference. The company’s strategy combines **innovative custom AI chips** (“Wormhole” first-generation PCIe cards) and a **fully open-source software stack** to deliver high-performance, scalable LLM inference. Tenstorrent’s hardware (based on proprietary _Tensix_ cores) has been demonstrated running state-of-the-art models like Meta’s LLaMA-70B and Alibaba’s Qwen-72B, both in single-node workstations and multi-board clusters. By 2025, Tenstorrent has open-sourced major components of its inference stack – from low-level kernel libraries to model serving frameworks – aiming to cultivate an ecosystem that can compete with Nvidia’s CUDA in flexibility and performance. This report provides a comprehensive technical deep dive into Tenstorrent’s LLM inference efforts, covering the hardware architecture, core software repositories, kernel and compiler optimizations, supported models, performance benchmarks, and future roadmap.

<div align="center">
    <img src="images/Metalium-vs-TTNN.webp" alt="ttnn + tt-metal" title="ttnn + tt-metal"/>
    <p><em>ttnn + tt-metal</em></p>
</div>

## Inference Strategy and Goals

**Open Hardware Access and Performance:** Tenstorrent’s inference strategy centers on exposing the full capabilities of its AI chips to developers. To that end, the company provides a **bare-metal programming model (“Metalium”)** that allows expert users to write custom kernels and orchestrate data movement at the lowest level. This is akin to a “CUDA for Tenstorrent,” enabling fine-grained performance tuning by directly controlling the _Tensix_ cores and on-chip memory. While only a small subset of users will write custom kernels, Tenstorrent views this openness as vital for squeezing out efficiency in production deployments. In parallel, Tenstorrent offers higher-level APIs and automation (via its TT-NN library and compiler stack) so that most users can run models out-of-the-box without dealing with hardware details.

**Scalable Multi-Chip Inference:** A key goal is to run **very large models and many concurrent queries** by scaling across multiple chips. Tenstorrent’s first-gen Wormhole cards were designed for scale-out: each chip includes **16× 100 Gbps interconnect lanes** that allow building multi-card meshes (the 32-chip _Galaxy_ system) without external switches. The inference software supports _tensor parallelism_ to shard giant models across devices and _data parallelism_ to serve many requests in parallel. For example, LLaMA-70B has been run with **tensor-parallel 8-way splitting on 8 cards**, and Falcon-40B was demonstrated on a **32-chip Galaxy** server. This scale-out approach lets Tenstorrent treat a cluster of chips as one large memory and compute resource for LLMs – the Galaxy appliance is essentially pitched as an **LLM inference server** in a box.

<div align="center">
    <img src="images/Galaxy-system.webp" alt="Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh" title="Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh"/>
    <p><em>Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh</em></p>
</div>

**High-Throughput, Acceptable Latency:** Tenstorrent optimizes for high throughput (tokens/sec) under multi-user server loads while keeping latency per user reasonable. Internally, they use the metric _tokens per second per user_ (t/s/u), i.e. how fast each user receives tokens given a certain batch of concurrent users. The company considers **≥10 tokens/sec/user** sufficient for responsive chat applications. Their **Loud Box** workstation (8 Wormhole cards) currently achieves \~15 tokens/s per user with 32 users on LLaMA-70B, which is already in the interactive range. This performance is approaching the ballpark of Nvidia H100 systems (cloud APIs on 8×H100 report \~20–50 tokens/s/user). Tenstorrent’s near-term goal is to **double single-user speed via software optimizations** (e.g. improved kernels, speculative decoding) on the same hardware, and to further close the gap with second-gen chips. At the system level, Tenstorrent positions itself as a cost-efficient alternative – their 8-card workstation (\~\$12k) targets developers who need on-premise LLM serving, compared to the expensive DGX boxes used by hyperscalers.

**Framework Integration and Ease of Use:** Acknowledging that most AI engineers work with established frameworks, Tenstorrent’s stack is designed to integrate with front-ends like PyTorch, TensorFlow, ONNX, and even LLM-specific serving frameworks. The strategy involves a **top-down compiler** (to import models from various frameworks and optimize them for Tenstorrent chips) and a **runtime that can plug into inference servers** (so that using a Tenstorrent back-end is as seamless as swapping a device type). For instance, Tenstorrent built a custom fork of the popular high-throughput serving engine **vLLM** to interface with their hardware, enabling features like continuous batching and efficient memory management for LLM workloads on Tenstorrent devices. They also provide an [**Inference Server repository**](https://github.com/tenstorrent/tt-inference-server) with ready-made model API implementations and Docker images, aiming to let users deploy common models on Tenstorrent cards with minimal setup. Overall, Tenstorrent’s inference strategy is to combine **open, low-level control for maximum performance** with **easy adoption paths via high-level tools**, thereby appealing both to performance-tuning experts and to AI teams wanting a plug-and-play solution.
