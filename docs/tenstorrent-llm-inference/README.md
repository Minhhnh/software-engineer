# Tenstorrent LLM Inference – Technical Deep Dive

Tenstorrent is an AI hardware startup building next-generation neural processors, with a strong focus on large language model (LLM) inference. The company’s strategy combines **innovative custom AI chips** (“Wormhole” first-generation PCIe cards) and a **fully open-source software stack** to deliver high-performance, scalable LLM inference. Tenstorrent’s hardware (based on proprietary _Tensix_ cores) has been demonstrated running state-of-the-art models like Meta’s LLaMA-70B and Alibaba’s Qwen-72B, both in single-node workstations and multi-board clusters. By 2025, Tenstorrent has open-sourced major components of its inference stack – from low-level kernel libraries to model serving frameworks – aiming to cultivate an ecosystem that can compete with Nvidia’s CUDA in flexibility and performance. This report provides a comprehensive technical deep dive into Tenstorrent’s LLM inference efforts, covering the hardware architecture, core software repositories, kernel and compiler optimizations, supported models, performance benchmarks, and future roadmap.

<div align="center">
    <img src="images/Metalium-vs-TTNN.webp" alt="ttnn + tt-metal" title="ttnn + tt-metal"/>
    <p><em>ttnn + tt-metal</em></p>
</div>

## Inference Strategy and Goals

**Open Hardware Access and Performance:** Tenstorrent’s inference strategy centers on exposing the full capabilities of its AI chips to developers. To that end, the company provides a **bare-metal programming model (“Metalium”)** that allows expert users to write custom kernels and orchestrate data movement at the lowest level. This is akin to a “CUDA for Tenstorrent,” enabling fine-grained performance tuning by directly controlling the _Tensix_ cores and on-chip memory. While only a small subset of users will write custom kernels, Tenstorrent views this openness as vital for squeezing out efficiency in production deployments. In parallel, Tenstorrent offers higher-level APIs and automation (via its TT-NN library and compiler stack) so that most users can run models out-of-the-box without dealing with hardware details.

**Scalable Multi-Chip Inference:** A key goal is to run **very large models and many concurrent queries** by scaling across multiple chips. Tenstorrent’s first-gen Wormhole cards were designed for scale-out: each chip includes **16× 100 Gbps interconnect lanes** that allow building multi-card meshes (the 32-chip _Galaxy_ system) without external switches. The inference software supports _tensor parallelism_ to shard giant models across devices and _data parallelism_ to serve many requests in parallel. For example, LLaMA-70B has been run with **tensor-parallel 8-way splitting on 8 cards**, and Falcon-40B was demonstrated on a **32-chip Galaxy** server. This scale-out approach lets Tenstorrent treat a cluster of chips as one large memory and compute resource for LLMs – the Galaxy appliance is essentially pitched as an **LLM inference server** in a box.

<div align="center">
    <img src="images/Galaxy-system.webp" alt="Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh" title="Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh"/>
    <p><em>Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh</em></p>
</div>

**High-Throughput, Acceptable Latency:** Tenstorrent optimizes for high throughput (tokens/sec) under multi-user server loads while keeping latency per user reasonable. Internally, they use the metric _tokens per second per user_ (t/s/u), i.e. how fast each user receives tokens given a certain batch of concurrent users. The company considers **≥10 tokens/sec/user** sufficient for responsive chat applications. Their **Loud Box** workstation (8 Wormhole cards) currently achieves \~15 tokens/s per user with 32 users on LLaMA-70B, which is already in the interactive range. This performance is approaching the ballpark of Nvidia H100 systems (cloud APIs on 8×H100 report \~20–50 tokens/s/user). Tenstorrent’s near-term goal is to **double single-user speed via software optimizations** (e.g. improved kernels, speculative decoding) on the same hardware, and to further close the gap with second-gen chips. At the system level, Tenstorrent positions itself as a cost-efficient alternative – their 8-card workstation (\~\$12k) targets developers who need on-premise LLM serving, compared to the expensive DGX boxes used by hyperscalers.

**Framework Integration and Ease of Use:** Acknowledging that most AI engineers work with established frameworks, Tenstorrent’s stack is designed to integrate with front-ends like PyTorch, TensorFlow, ONNX, and even LLM-specific serving frameworks. The strategy involves a **top-down compiler** (to import models from various frameworks and optimize them for Tenstorrent chips) and a **runtime that can plug into inference servers** (so that using a Tenstorrent back-end is as seamless as swapping a device type). For instance, Tenstorrent built a custom fork of the popular high-throughput serving engine **vLLM** to interface with their hardware, enabling features like continuous batching and efficient memory management for LLM workloads on Tenstorrent devices. They also provide an [**Inference Server repository**](https://github.com/tenstorrent/tt-inference-server) with ready-made model API implementations and Docker images, aiming to let users deploy common models on Tenstorrent cards with minimal setup. Overall, Tenstorrent’s inference strategy is to combine **open, low-level control for maximum performance** with **easy adoption paths via high-level tools**, thereby appealing both to performance-tuning experts and to AI teams wanting a plug-and-play solution.

## Key Repositories and Software Stack

Tenstorrent’s LLM inference software stack spans several actively maintained open-source repositories, each addressing different layers of the stack:

<div align="center">
    <img src="images/tenstorrent_software_stack.webp" alt="Tenstorrent’s software stack" title="Tenstorrent’s software stack"/>
    <p><em>Tenstorrent’s software stack</em></p>
</div>

- **TT-Metal (TT-NN and TT-Metalium)** – _Operator Library and Kernel Runtime:_ This is the core Python/C++ library for executing neural network operations on Tenstorrent hardware. **TT-NN** provides a tensor class (`ttnn.Tensor`) and dozens of operators (akin to PyTorch’s ATen or CUDA’s cuDNN) implemented for the Tenstorrent architecture. It allows creating and running computation graphs on the device using familiar tensor semantics. Under the hood, TT-NN builds on **TT-Metalium**, Tenstorrent’s low-level programming model for the hardware. TT-Metalium exposes the _Tensix_ core architecture in a C++ API, letting developers write custom device kernels by orchestrating the five RISC-V sub-cores, DMA engines, and matrix units (more details in the next section). In practice, most developers use the high-level TT-NN ops, while a few expert users or Tenstorrent engineers implement new optimized kernels in Metalium when needed. The TT-Metal repository includes extensive documentation and even _tech reports_ on topics like the Matrix Engine and data formats. It also contains _model demo scripts_ for various networks and a performance table (updated as of May 5, 2025) showcasing throughput and latency across models and hardware configurations. TT-Metal is under active development with thousands of commits and a robust issues/pull-requests workflow, reflecting its central role in Tenstorrent’s software. (Notably, Tenstorrent uses the term “TT-Metal” sometimes to refer broadly to this whole stack, but technically TT-Metalium is the metal-level API and TT-NN is the neural network library.)

- **TT-Forge (MLIR-Based Compiler)** – _Graph Compiler and Runtime Planner:_ TT-Forge is Tenstorrent’s newer compiler framework (successor to an earlier stack called _Buda_). It is built on LLVM’s MLIR and serves as a **high-level compiler that ingests models from popular frameworks and lowers them to Tenstorrent executable code**. The idea is to enable a model trained in PyTorch or TensorFlow to be deployed on Tenstorrent silicon with automated graph optimization, placement, and scheduling. TT-Forge’s front-end (tt-forge-fe) can take in PyTorch JIT IR, ONNX graphs, etc., and perform transformations like operator fusion, layout conversion, and graph rewriting for performance. The compiler then leverages **TT-MLIR** passes to target the Tensix core architecture. The output is a low-level “netlist” or program that maps each neural net operation to specific Tensix cores and schedules data movement – a human-readable form of this mapping is available for inspection before it’s assembled into a binary for execution. TT-Forge is intended to be general and open, supporting all configurations of Tenstorrent hardware via a single, performance-oriented compiler. In essence, TT-Forge and its components automate what a skilled developer might otherwise do by hand with TT-Metalium: partitioning the model across cores/devices, choosing tiling strategies, and inserting data transfers. It’s actively updated (as of May 2025) and is a key piece for scaling to new models and future chips.

- **PyTorch Integration (torch.compile backend)** – Tenstorrent maintains an integration with PyTorch 2.x through a project often referred to as **pytorch2.0_ttnn**. This provides a backend for PyTorch’s `torch.compile()` (AOT compilation) so that users can take a PyTorch model and just compile it for Tenstorrent, similar to how one would for XLA or other accelerators. It uses the TT-NN compiler under the hood (TT-Forge/MLIR) to translate PyTorch ops into TT-Metal ops and kernels. By supporting PyTorch’s compiler API, Tenstorrent enables researchers to test models on their hardware with minimal code changes – essentially treat the Tenstorrent card like another device in the PyTorch ecosystem. Alongside this, Tenstorrent also has a **PJRT device for XLA/JAX** (the `tt-xla` repo) and a **TVM integration** (`tt-tvm` repo), underscoring their approach of meeting developers where they are. These compiler integrations are crucial for leveraging Tenstorrent hardware in existing ML workflows for both training and inference, although in practice by 2025 the emphasis is on inference deployment.

- **TT-Inference-Server** – _Model Serving APIs and vLLM Integration:_ This repository provides ready-to-use server implementations for popular LLMs on Tenstorrent hardware. It essentially packages **a modified vLLM** (open-source LLM serving engine) with Tenstorrent-specific backend hooks, along with model-specific configurations and API endpoints. The included **“Model Readiness” table** lists each supported model, a link to its Hugging Face weights, the recommended Tenstorrent hardware for it, and the software version (TT-Metal and vLLM commit) it’s compatible with. For example, entries include _Llama-3.3-70B-Instruct_ on a LoudBox (8 cards) marked “ready,” or _Qwen2.5-7B_ on a single n150 card, etc.. Each model name in the table links to an implementation – typically a small Python module defining how to load the model weights into TT-NN, any model-specific pre/post-processing, and integration with vLLM’s engine. The server leverages **vLLM’s high-throughput scheduler** (which uses a “paged attention” mechanism to efficiently handle concurrent requests) but replaces GPU-specific operations with calls into TT-NN (TT-Metal ops). In effect, TT-Inference-Server + vLLM gives users a **HTTP/GRPC serving interface** for LLM inference on Tenstorrent, with features like dynamic batching, streaming token output, etc., similar to a HuggingFace TextGenerationInference server but optimized for Tenstorrent. The repository also provides **Docker containers** for quick deployment of these inference servers. This component underscores Tenstorrent’s focus on _actively maintained, ready-to-run inference solutions_ – one can spin up a container and serve a model on a Tenstorrent box without having to write any kernel code or graph optimization by hand.

- **Support Utilities:** In addition to the main stack above, Tenstorrent provides tools like **TT-SMI** (a CLI tool similar to `nvidia-smi` for monitoring cards), **TT-Topology** for multi-card ethernet topology configuration, firmware flashing tools (TT-Flash), and visualization/debug tools (e.g. **ttnn-visualizer** for graph and memory visualization). These support inference deployment by giving insight into performance and helping manage hardware. For debugging or development, a **TTNN “graph trace” and profiling** feature exists to inspect how ops are executed on Tensix cores. Tenstorrent also maintains example **model zoo repos** (like `tt-buda-demos` and in TT-Metal’s docs) that show how to run various models (CNNs, Transformers, Whisper, Stable Diffusion, etc.) with their stack – many of these double as integration tests and performance benchmarks for inference.

In summary, Tenstorrent’s inference stack ranges from _low-level (Metalium ISA, custom kernels)_ to _high-level (model servers and PyTorch integration)_, with each layer open-sourced. The actively maintained repos ensure that new models and techniques can be rapidly incorporated. For instance, when Meta released Llama 3.2/3.3 models in late 2024, Tenstorrent’s team updated the model table and added support for Vision and Instruct variants, illustrating the agility of an open-source approach. Table-driven model support, combined with a robust compiler and core library, makes it possible to extend support to other architectures (they even issued bounties for community contributions to add models like _Gemini_ and _Phi-2_ to TT-Buda/Forge). This flexible software foundation is crucial for handling the fast-moving LLM landscape.
