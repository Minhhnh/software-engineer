# Tenstorrent LLM Inference – Technical Deep Dive

Tenstorrent is an AI hardware startup building next-generation neural processors, with a strong focus on large language model (LLM) inference. The company’s strategy combines **innovative custom AI chips** (“Wormhole” first-generation PCIe cards) and a **fully open-source software stack** to deliver high-performance, scalable LLM inference. Tenstorrent’s hardware (based on proprietary _Tensix_ cores) has been demonstrated running state-of-the-art models like Meta’s LLaMA-70B and Alibaba’s Qwen-72B, both in single-node workstations and multi-board clusters. By 2025, Tenstorrent has open-sourced major components of its inference stack – from low-level kernel libraries to model serving frameworks – aiming to cultivate an ecosystem that can compete with Nvidia’s CUDA in flexibility and performance. This report provides a comprehensive technical deep dive into Tenstorrent’s LLM inference efforts, covering the hardware architecture, core software repositories, kernel and compiler optimizations, supported models, performance benchmarks, and future roadmap.

<div align="center">
    <img src="images/Metalium-vs-TTNN.webp" alt="ttnn + tt-metal" title="ttnn + tt-metal"/>
    <p><em>ttnn + tt-metal</em></p>
</div>

## Inference Strategy and Goals

**Open Hardware Access and Performance:** Tenstorrent’s inference strategy centers on exposing the full capabilities of its AI chips to developers. To that end, the company provides a **bare-metal programming model (“Metalium”)** that allows expert users to write custom kernels and orchestrate data movement at the lowest level. This is akin to a “CUDA for Tenstorrent,” enabling fine-grained performance tuning by directly controlling the _Tensix_ cores and on-chip memory. While only a small subset of users will write custom kernels, Tenstorrent views this openness as vital for squeezing out efficiency in production deployments. In parallel, Tenstorrent offers higher-level APIs and automation (via its TT-NN library and compiler stack) so that most users can run models out-of-the-box without dealing with hardware details.

**Scalable Multi-Chip Inference:** A key goal is to run **very large models and many concurrent queries** by scaling across multiple chips. Tenstorrent’s first-gen Wormhole cards were designed for scale-out: each chip includes **16× 100 Gbps interconnect lanes** that allow building multi-card meshes (the 32-chip _Galaxy_ system) without external switches. The inference software supports _tensor parallelism_ to shard giant models across devices and _data parallelism_ to serve many requests in parallel. For example, LLaMA-70B has been run with **tensor-parallel 8-way splitting on 8 cards**, and Falcon-40B was demonstrated on a **32-chip Galaxy** server. This scale-out approach lets Tenstorrent treat a cluster of chips as one large memory and compute resource for LLMs – the Galaxy appliance is essentially pitched as an **LLM inference server** in a box.

<div align="center">
    <img src="images/Galaxy-system.webp" alt="Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh" title="Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh"/>
    <p><em>Tenstorrent’s Galaxy system comprises 32 Wormhole chips connected in a mesh</em></p>
</div>

**High-Throughput, Acceptable Latency:** Tenstorrent optimizes for high throughput (tokens/sec) under multi-user server loads while keeping latency per user reasonable. Internally, they use the metric _tokens per second per user_ (t/s/u), i.e. how fast each user receives tokens given a certain batch of concurrent users. The company considers **≥10 tokens/sec/user** sufficient for responsive chat applications. Their **Loud Box** workstation (8 Wormhole cards) currently achieves \~15 tokens/s per user with 32 users on LLaMA-70B, which is already in the interactive range. This performance is approaching the ballpark of Nvidia H100 systems (cloud APIs on 8×H100 report \~20–50 tokens/s/user). Tenstorrent’s near-term goal is to **double single-user speed via software optimizations** (e.g. improved kernels, speculative decoding) on the same hardware, and to further close the gap with second-gen chips. At the system level, Tenstorrent positions itself as a cost-efficient alternative – their 8-card workstation (\~\$12k) targets developers who need on-premise LLM serving, compared to the expensive DGX boxes used by hyperscalers.

**Framework Integration and Ease of Use:** Acknowledging that most AI engineers work with established frameworks, Tenstorrent’s stack is designed to integrate with front-ends like PyTorch, TensorFlow, ONNX, and even LLM-specific serving frameworks. The strategy involves a **top-down compiler** (to import models from various frameworks and optimize them for Tenstorrent chips) and a **runtime that can plug into inference servers** (so that using a Tenstorrent back-end is as seamless as swapping a device type). For instance, Tenstorrent built a custom fork of the popular high-throughput serving engine **vLLM** to interface with their hardware, enabling features like continuous batching and efficient memory management for LLM workloads on Tenstorrent devices. They also provide an [**Inference Server repository**](https://github.com/tenstorrent/tt-inference-server) with ready-made model API implementations and Docker images, aiming to let users deploy common models on Tenstorrent cards with minimal setup. Overall, Tenstorrent’s inference strategy is to combine **open, low-level control for maximum performance** with **easy adoption paths via high-level tools**, thereby appealing both to performance-tuning experts and to AI teams wanting a plug-and-play solution.

## Key Repositories and Software Stack

Tenstorrent’s LLM inference software stack spans several actively maintained open-source repositories, each addressing different layers of the stack:

<div align="center">
    <img src="images/tenstorrent_software_stack.webp" alt="Tenstorrent’s software stack" title="Tenstorrent’s software stack"/>
    <p><em>Tenstorrent’s software stack</em></p>
</div>

- **TT-Metal (TT-NN and TT-Metalium)** – _Operator Library and Kernel Runtime:_ This is the core Python/C++ library for executing neural network operations on Tenstorrent hardware. **TT-NN** provides a tensor class (`ttnn.Tensor`) and dozens of operators (akin to PyTorch’s ATen or CUDA’s cuDNN) implemented for the Tenstorrent architecture. It allows creating and running computation graphs on the device using familiar tensor semantics. Under the hood, TT-NN builds on **TT-Metalium**, Tenstorrent’s low-level programming model for the hardware. TT-Metalium exposes the _Tensix_ core architecture in a C++ API, letting developers write custom device kernels by orchestrating the five RISC-V sub-cores, DMA engines, and matrix units (more details in the next section). In practice, most developers use the high-level TT-NN ops, while a few expert users or Tenstorrent engineers implement new optimized kernels in Metalium when needed. The TT-Metal repository includes extensive documentation and even _tech reports_ on topics like the Matrix Engine and data formats. It also contains _model demo scripts_ for various networks and a performance table (updated as of May 5, 2025) showcasing throughput and latency across models and hardware configurations. TT-Metal is under active development with thousands of commits and a robust issues/pull-requests workflow, reflecting its central role in Tenstorrent’s software. (Notably, Tenstorrent uses the term “TT-Metal” sometimes to refer broadly to this whole stack, but technically TT-Metalium is the metal-level API and TT-NN is the neural network library.)

- **TT-Forge (MLIR-Based Compiler)** – _Graph Compiler and Runtime Planner:_ TT-Forge is Tenstorrent’s newer compiler framework (successor to an earlier stack called _Buda_). It is built on LLVM’s MLIR and serves as a **high-level compiler that ingests models from popular frameworks and lowers them to Tenstorrent executable code**. The idea is to enable a model trained in PyTorch or TensorFlow to be deployed on Tenstorrent silicon with automated graph optimization, placement, and scheduling. TT-Forge’s front-end (tt-forge-fe) can take in PyTorch JIT IR, ONNX graphs, etc., and perform transformations like operator fusion, layout conversion, and graph rewriting for performance. The compiler then leverages **TT-MLIR** passes to target the Tensix core architecture. The output is a low-level “netlist” or program that maps each neural net operation to specific Tensix cores and schedules data movement – a human-readable form of this mapping is available for inspection before it’s assembled into a binary for execution. TT-Forge is intended to be general and open, supporting all configurations of Tenstorrent hardware via a single, performance-oriented compiler. In essence, TT-Forge and its components automate what a skilled developer might otherwise do by hand with TT-Metalium: partitioning the model across cores/devices, choosing tiling strategies, and inserting data transfers. It’s actively updated (as of May 2025) and is a key piece for scaling to new models and future chips.

- **PyTorch Integration (torch.compile backend)** – Tenstorrent maintains an integration with PyTorch 2.x through a project often referred to as **pytorch2.0_ttnn**. This provides a backend for PyTorch’s `torch.compile()` (AOT compilation) so that users can take a PyTorch model and just compile it for Tenstorrent, similar to how one would for XLA or other accelerators. It uses the TT-NN compiler under the hood (TT-Forge/MLIR) to translate PyTorch ops into TT-Metal ops and kernels. By supporting PyTorch’s compiler API, Tenstorrent enables researchers to test models on their hardware with minimal code changes – essentially treat the Tenstorrent card like another device in the PyTorch ecosystem. Alongside this, Tenstorrent also has a **PJRT device for XLA/JAX** (the `tt-xla` repo) and a **TVM integration** (`tt-tvm` repo), underscoring their approach of meeting developers where they are. These compiler integrations are crucial for leveraging Tenstorrent hardware in existing ML workflows for both training and inference, although in practice by 2025 the emphasis is on inference deployment.

- **TT-Inference-Server** – _Model Serving APIs and vLLM Integration:_ This repository provides ready-to-use server implementations for popular LLMs on Tenstorrent hardware. It essentially packages **a modified vLLM** (open-source LLM serving engine) with Tenstorrent-specific backend hooks, along with model-specific configurations and API endpoints. The included **“Model Readiness” table** lists each supported model, a link to its Hugging Face weights, the recommended Tenstorrent hardware for it, and the software version (TT-Metal and vLLM commit) it’s compatible with. For example, entries include _Llama-3.3-70B-Instruct_ on a LoudBox (8 cards) marked “ready,” or _Qwen2.5-7B_ on a single n150 card, etc.. Each model name in the table links to an implementation – typically a small Python module defining how to load the model weights into TT-NN, any model-specific pre/post-processing, and integration with vLLM’s engine. The server leverages **vLLM’s high-throughput scheduler** (which uses a “paged attention” mechanism to efficiently handle concurrent requests) but replaces GPU-specific operations with calls into TT-NN (TT-Metal ops). In effect, TT-Inference-Server + vLLM gives users a **HTTP/GRPC serving interface** for LLM inference on Tenstorrent, with features like dynamic batching, streaming token output, etc., similar to a HuggingFace TextGenerationInference server but optimized for Tenstorrent. The repository also provides **Docker containers** for quick deployment of these inference servers. This component underscores Tenstorrent’s focus on _actively maintained, ready-to-run inference solutions_ – one can spin up a container and serve a model on a Tenstorrent box without having to write any kernel code or graph optimization by hand.

- **Support Utilities:** In addition to the main stack above, Tenstorrent provides tools like **TT-SMI** (a CLI tool similar to `nvidia-smi` for monitoring cards), **TT-Topology** for multi-card ethernet topology configuration, firmware flashing tools (TT-Flash), and visualization/debug tools (e.g. **ttnn-visualizer** for graph and memory visualization). These support inference deployment by giving insight into performance and helping manage hardware. For debugging or development, a **TTNN “graph trace” and profiling** feature exists to inspect how ops are executed on Tensix cores. Tenstorrent also maintains example **model zoo repos** (like `tt-buda-demos` and in TT-Metal’s docs) that show how to run various models (CNNs, Transformers, Whisper, Stable Diffusion, etc.) with their stack – many of these double as integration tests and performance benchmarks for inference.

In summary, Tenstorrent’s inference stack ranges from _low-level (Metalium ISA, custom kernels)_ to _high-level (model servers and PyTorch integration)_, with each layer open-sourced. The actively maintained repos ensure that new models and techniques can be rapidly incorporated. For instance, when Meta released Llama 3.2/3.3 models in late 2024, Tenstorrent’s team updated the model table and added support for Vision and Instruct variants, illustrating the agility of an open-source approach. Table-driven model support, combined with a robust compiler and core library, makes it possible to extend support to other architectures (they even issued bounties for community contributions to add models like _Gemini_ and _Phi-2_ to TT-Buda/Forge). This flexible software foundation is crucial for handling the fast-moving LLM landscape.

## Architectural and Kernel Optimizations

_Hardware Overview:_ Tenstorrent’s LLM inference optimizations are deeply tied to the design of its **Tensix cores** and memory architecture. A Tensix core is a **spatially-programmable compute unit** with independent control, rather than a SIMD lane or fixed-function MAC array. Inside each core are multiple RISC-V micro-cores and specialized engines: _two RISC-V “router” cores_ handle data movement (DMA and NoC transfers), and _three RISC-V “compute” cores_ issue vector and matrix-multiply instructions to the core’s math units. The math units consist of an FPU (which in Tenstorrent terminology is actually a **matrix multiply engine**) and an SFPU (a **vector/SIMD engine** for elementwise ops). Each core also includes a chunk of **SRAM (L1 memory)**, which is tightly coupled. The first-gen chips (_Grayskull_ and _Wormhole_) have on the order of 80–120 Tensix cores and \~100+ MB of total on-chip SRAM. These cores are connected by a high-bandwidth, circuit-switched network-on-chip. Importantly, in the [Wormhole](https://tenstorrent.com/en/hardware/wormhole) design, multiple chips can be directly linked via 100 Gbps Ethernet-style links and **coherent memory networking** – a core on one chip can fetch data from another chip’s SRAM almost as if it were local. This architecture is optimized for **fine-grained parallelism and pipeline parallelism**. Unlike a GPU where hundreds of threads execute in lock-step, Tensix cores run **asynchronously (MIMD)** – each core can be working on a different part of the computation or even on a different layer of the model, communicating via on-chip routers when needed. This affords a lot of flexibility in mapping an LLM’s computation graph onto the hardware.

<div align="center">
    <img src="images/Tensix-core.webp" alt="Inside a Tenstorrent Tensix core, BRISC and NRISC handle data movement into and out of the core, while TRISC0, TRISC1, and TRISC2 manage the math engines. The FPU serves as a matrix-multiply accelerator (despite its name suggesting a floating-point unit), and the SFPU functions as a vector engine." title="Inside a Tenstorrent Tensix core, BRISC and NRISC handle data movement into and out of the core, while TRISC0, TRISC1, and TRISC2 manage the math engines. The FPU serves as a matrix-multiply accelerator (despite its name suggesting a floating-point unit), and the SFPU functions as a vector engine."/>
    <p><em>Inside a Tenstorrent Tensix core, BRISC and NRISC handle data movement into and out of the core, while TRISC0, TRISC1, and TRISC2 manage the math engines. The FPU serves as a matrix-multiply accelerator (despite its name suggesting a floating-point unit), and the SFPU functions as a vector engine.</em></p>
</div>

_Kernel (Metalium) Execution Model:_ To keep the many MAC units busy, Tenstorrent relies on a **decoupled execution model** – separating computation from data movement. As VP of HW Matt Mattina explained, _“math is separate from unpacking and packing data”_, and both run in parallel. In practice, this means a TT-Metalium kernel might dedicate some Tensix cores solely to staging data (bringing in next tokens’ data from DRAM or distributing key/value cache across chips) while other cores multiply matrices, and yet others move results around. The TT-Metalium API allows explicit scheduling of work to RISC-V threads on each core, rather than relying on implicit caching or global barriers. For example, a custom attention kernel could use one thread to load query and key tiles into SRAM, another to compute partial dot products (matrix multiplies), and a third to stream out the computed attention scores – all overlapping in time. This is reminiscent of manually orchestrating pipeline parallelism at the kernel level, which Tenstorrent enables through Metalium. Such **explicit control of dataflow** is crucial given the distributed memory: it ensures that the theoretical peak of \~2,000 multiply-accumulates per cycle per core can be approached by keeping data moving continuously. In contrast to CUDA where warps are implicitly synchronized each instruction, Metalium lets each core run its own instruction stream, only synchronizing when and where the programmer wants. This freedom allows implementing _models with different parallelization strategies_ – e.g. one could assign a subset of cores to each transformer layer (pipeline the layers through core groups), or do each matrix multiply across many cores in parallel, or combinations thereof. Tenstorrent’s compiler (TT-Forge) typically handles this mapping automatically, but advanced users can tweak the placement via the netlist if needed.

**Data Layouts and Memory Optimization:** One notable optimization in Tenstorrent’s inference stack is the use of **custom tensor layouts** to match hardware. Tenstorrent does _not_ use standard row-major order in memory for multi-dimensional tensors, because their hardware benefits from tiling data to fit SRAM and align with matrix engine dimensions. For instance, they use a tiled layout for matrices so that blocks of the matrix reside in contiguous SRAM segments (enabling better locality when cores multiply sub-blocks). This non-row-major format improves on-chip reuse but required significant work in the software: the TT-NN library must transform (“format”) incoming weights/activations into the tiled layout expected by kernels. The TT-NN API includes utilities like `ttnn.format_input_tensor()` and `pad_to_tile_shape()` to help prepare data. A developer integrating Tenstorrent with a new framework observed that _“the use of non row-major layout is the main troublemaker… Everyone assumes each other speaks row-major, so an adapting interface has to be made”_. This highlights a challenge: while these layouts boost performance, they necessitate careful data conversion at boundaries (e.g. when copying weights from CPU memory into the accelerator’s memory). Tenstorrent’s solution is to handle this transparently in their compiler/loader – when a model is imported via PyBuda or TT-Forge, it automatically converts the weight matrices into the optimal internal format. At runtime, **all operations on Tenstorrent devices expect and produce tensors in these native layouts**. This can complicate mixing Tenstorrent with other accelerators in a pipeline (due to conversion overhead), but it ensures that _within_ the Tenstorrent execution, memory access is efficient. The benefit is evident in performance scaling: even at very high batch sizes or with many cores active, the chip is not starved for data because the data movement threads and tiling keep caches warm and memory accesses coalesced.

**Precision and Numeric Optimizations:** In LLM inference, using lower numerical precision can significantly speed up throughput and reduce memory usage. Tenstorrent heavily leverages **8-bit floating point** formats for inference. The Wormhole chip supports FP16/BF16 as well, but its peak theoretical is 74 TFLOPS (FP16) versus 262 TFLOPS in _FP8_ on an n150 card. In practice, Tenstorrent runs large LLMs at **Block FP8 (BFLOAT8)** precision for generation. Their BF8 format is a kind of block-floating scheme (shared exponent) that balances range and precision. Using BF8, a 70B model’s weights are roughly half the size of BF16, enabling a model like Llama-70B to fit on 8 cards (each card with 12 GB GDDR6) with room for activation memory. The TT-Metal software handles weight quantization to BF8 and any required dequantization. They note in documentation a _limitation of BFLOAT8_B_ (likely minor accuracy considerations), but overall have found it effective for LLMs. Empirically, running Llama-70B in BF8 on Tenstorrent produced coherent outputs in their demos (the model quality remained high). Beyond FP8, Tenstorrent cores also support _int8_ and other low-precision dot products – though primarily for vision models or experimentation. The software allows mixing precision: for example, one could keep embedding layers in BF16 and transformer blocks in BF8 if needed for accuracy. However, **nearly all their published LLM benchmark numbers use 8-bit** arithmetic for maximum throughput. This is a conscious optimization to compete with Nvidia’s TensorRT and FasterTransformer libraries, which also use 8-bit or 4-bit quantization for serving large models. Tenstorrent’s advantage is that BF8 is natively supported in hardware (no extra latency for dequantization on tensor cores, etc.), so the throughput scales almost linearly with the reduced precision. Kernel-wise, Tenstorrent likely implemented custom **matrix multiplication kernels** for BF8 in Metalium to accumulate into higher precision (similar to how Nvidia does FP8 => accumulate in FP16). The performance table confirms massive gains: e.g. Llama3.1-70B at BF8 on 8 chips hits \~15 tokens/s/user (as noted), which would be much lower if FP16 were used (they set a “target” of 20 tokens/s/user for 70B, presumably at BF8, whereas FP16 might only reach half that).

**Multi-Device Parallelism:** On the software side, Tenstorrent’s inference stack supports **Tensor Parallel (TP)** and **Data Parallel (DP)** distribution seamlessly. In their [model performance table](https://github.com/tenstorrent/tt-metal?tab=readme-ov-file#llms), entries like “(TP=8)” indicate a model’s layers were split across 8 devices, each device holding 1/8 of the model weights. The TT-Forge compiler can partition the weight matrices accordingly and insert the necessary all-reduce or scatter/gather operations over the NoC/Ethernet links. Thanks to Wormhole’s built-in 100G ×16 links, the communication overhead for TP is relatively low – for instance, Qwen-72B with TP=8 on a QuietBox (8 cards) achieved \~14.5 tokens/s/user, which is in line with the Llama-70B numbers and suggests the scaling is efficient. For Data Parallel, Tenstorrent can broadcast a copy of the model to multiple devices and have each handle different batch slices (or different users). The table shows Falcon-7B scaled to DP=32 (across 32 chips) serving 1024 concurrent prompts at \~4.8 tokens/s each – an aggregate of nearly 5k tokens/s throughput. This demonstrates nearly linear scaling of throughput with more devices in data-parallel mode (32× increase in batch yielded \~32× total tokens/sec). Behind the scenes, such scaling is facilitated by _Tenstorrent’s graph compiler and runtime which manage synchronization and networking._ The **Wormhole chips treat remote memory access almost like another level of cache**, as noted, making it simpler to program model parallelism; e.g. a core can fetch weight chunks from a neighboring chip’s SRAM during a TP layer computation. The TT-Metal runtime abstracts this so that developers or the compiler can largely write parallel code without micromanaging low-level DMA for off-chip transfers – the hardware’s NIC and Metalium’s API can handle inter-chip copy operations (likely via special instructions or channels). This tight integration of compute and networking is a big architectural optimization for inference at scale, where multi-GPU setups often suffer from latency or limited interconnect bandwidth.

**Attention and Caching:** LLM inference is bottlenecked not only by matrix multiplies but also by the _attention mechanism_, which involves memory-intensive operations (e.g. large key/value caches). Tenstorrent’s approach to attention combines hardware memory advantages with software scheduling. Each Tensix core’s SRAM can act as a **scratchpad for storing a segment of the KV cache**. In a 80-core chip, the \~96 MB SRAM could hold a few thousand tokens of a 70B model’s keys and values (distributed across cores). For longer contexts (e.g. Qwen-72B supports 128K tokens), Tenstorrent likely streams the cache from off-chip GDDR6 as needed. The vLLM-based server uses a **“paged KV cache”** concept, which fits well with Tenstorrent’s architecture – it breaks the cache into blocks that can be loaded into SRAM on-demand. Since Tensix cores can individually fetch data, one can imagine cores cooperating such that when attention for token N is computed, cores pull only the relevant key blocks for the current query chunk, rather than keeping the entire 128K history on-chip. This resembles how vLLM handles GPU memory, but here the advantage is each core can directly access other chips’ SRAM over the NoC, effectively giving a larger pool of “unified” memory for KV. Additionally, Tenstorrent may have implemented **FlashAttention-like kernels** in Metalium (FlashAttention algorithm computes attention in tiled fashion to avoid storing full intermediate matrices). Given Metalium’s flexibility, a custom kernel could perform the Q·K^T multiply and softmax in chunks that fit in SRAM, eliminating large intermediate allocations. While specifics aren’t published, the high throughput for attention-heavy models suggests such optimizations. For instance, the _DeepSeek-R1 70B Distilled_ model (which likely has long-chain-of-thought prompts) still achieves \~15.2 tokens/s/user on 8 chips, indicating the attention step isn’t a major bottleneck there. Tenstorrent’s documentation notes that their reported t/s/u is measured **after the prompt prefill** (i.e., generation phase tokens/sec), which means the numbers account for attention + decode, and they are consistently high. This implies careful optimization of the attention cache accesses and compute. Another kernel-level trick is **speculative decoding** (generating multiple tokens ahead and verifying them), which Tenstorrent hasn’t implemented yet but is on their roadmap to further boost throughput and latency.

In summary, Tenstorrent’s architectural and kernel optimizations for LLM inference include: **(1)** decoupled execution with parallel data movement to keep compute busy, **(2)** custom tensor layouts and tiling to maximize on-chip SRAM re-use, **(3)** aggressive use of FP8 precision to increase effective compute and memory bandwidth, **(4)** built-in model/data parallel scaling with fast interconnects and a compiler that partitions models across cores/devices, and **(5)** specialized handling of attention and long context via paged caches and possibly fused kernels. These efforts collectively allow Tenstorrent hardware to approach state-of-the-art inference speeds. A notable point is that **Tenstorrent’s stack is still evolving** – the company acknowledges current software isn’t fully utilizing the hardware yet (they see room to double performance via software). The open Metalium model is a double-edged sword: it provides unmatched control, but writing optimal kernels is complex (the programming model is unlike CUDA and took developers time to learn). Tenstorrent is mitigating this by improving the compiler and providing more pre-optimized kernels for common patterns (they note “3 or 4 design patterns will cover 90% of use cases” in custom kernels). Over time, as the software matures, we can expect further tuning – e.g. more of the manual best practices (overlap compute/communication, etc.) will be automated by TT-Forge, and more ops (like various activation functions or layernorms) will get bespoke implementations to avoid any leftover inefficiencies.
