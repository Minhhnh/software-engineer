# TT-Inference-Server: Tenstorrent’s LLM Inference Server Overview

- https://github.com/tenstorrent/tt-inference-server

## Overall Purpose

**TT-Inference-Server** is Tenstorrent’s open-source inference server, designed to deploy Large Language Models (LLMs) (and some vision models) on Tenstorrent AI hardware. In essence, it provides a ready-to-run serving stack that integrates Tenstorrent’s custom accelerator libraries with high-level model APIs. The repository contains implementations of various model inference “APIs” (model handlers) which are optimized for Tenstorrent devices, so users can serve models on Tenstorrent’s **Wormhole** accelerators (e.g. **TT-LoudBox**, **TT-QuietBox** desktops or **n150** PCIe cards) with minimal setup. By abstracting the hardware details behind a standard interface (often an OpenAI-compatible REST API), TT-Inference-Server lets developers interact with Tenstorrent-powered models just like they would with a typical GPU-based server, but harnessing Tenstorrent’s efficiency.

## Architecture and Design

TT-Inference-Server’s architecture centers on a combination of Tenstorrent’s low-level libraries and an advanced LLM serving framework:

- **vLLM Integration:** At its core, TT-Inference-Server builds on _vLLM_, a high-performance inference engine for LLMs. It uses vLLM to handle model execution and request batching, thereby supporting features like fast **token streaming** and an OpenAI-like API for chat completions - [dstack.ai](https://dstack.ai/examples/accelerators/tenstorrent/). In fact, Tenstorrent provides a Dockerized “vLLM inference server” as part of this repo, which launches a vLLM-based API service pre-configured for their hardware. This means the server can accept chat/completion requests (via REST endpoints) and generate responses using the loaded model, benefiting from vLLM’s optimized scheduling of parallel requests.
- **TT-Metal and TT-NN Integration:** Under the hood, the server relies on Tenstorrent’s proprietary libraries – **TT-NN** and **TT-Metal** – to run neural network operations on the Tenstorrent chips. TT-NN is a Python/C++ operator library for standard deep learning ops, and TT-Metal (Metalium) is a low-level kernel programming model for the hardware. The inference server’s model implementations replace or augment default operations with TT-NN calls so that heavy compute (matrix multiplies, transformer attention, etc.) executes on Tenstorrent accelerators instead of CPU/GPU. In practice, TT-Inference-Server bridges vLLM with TT-Metal: when vLLM’s engine calls the model layers, those layers leverage TT-NN kernels optimized for Tenstorrent devices. This layered design keeps the high-level workflow (token generation, batching) in Python, while delegating compute-intensive ops to Tenstorrent’s highly parallel cores.
- **Modular Model Handlers:** The repository is structured to support multiple models through a consistent interface. For LLMs, there is a primary module (under `vllm-tt-metal-llama3/`) that contains the model definitions, weight-loading logic, and any custom kernels needed for models like Llama, Qwen, etc. Each supported model is registered with a configuration (model name, HuggingFace repo, required library versions) so the server knows [how to set it up](https://github.com/tenstorrent/tt-inference-server/releases). This modular approach makes it easier to add new models – typically by writing a new config entry or slight code adaptation if the architecture differs. For example, vision models like YOLOv4 are implemented in a separate module [(`tt-metal-yolov4`)](https://github.com/tenstorrent/tt-inference-server?tab=readme-ov-file#cnns) demonstrating the same pattern applied to CNNs.
- **Workflows and Scripts:** A notable design aspect is the concept of [**“workflows,”**](https://github.com/tenstorrent/tt-inference-server/blob/main/docs/workflows_user_guide.md) which are automated sequences for common tasks (setup, running the server, evaluation, etc.). The repo includes Python scripts to orchestrate these workflows. For instance, `run.py` serves as an entry-point that can invoke different workflow modes (starting a server, running evals, building Docker images, etc., depending on CLI args or environment). Similarly, dedicated scripts handle environment setup and container launch. This scripting layer abstracts away many manual steps, enabling one-command setup for supported models. The **Model Readiness Workflow** (documented in the repo) is an example, which automates pulling the correct model files and spinning up a containerized server. Internally, the code is organized with helper modules for things like virtual environment management and logging of each workflow step. Design patterns like this emphasize reproducibility and consistency – important given the variety of models and hardware configurations the server supports.

Overall, the architecture can be seen as a **two-layer system**: the upper layer (vLLM-based server and Python orchestration) handles model-agnostic logic – request handling, token generation loop, etc. – while the lower layer (TT-NN/Metal kernels and model-specific code) handles the model math on the Tenstorrent silicon. These layers communicate through well-defined interfaces, which is why the TT-Inference-Server can deliver a uniform API across many model types.
