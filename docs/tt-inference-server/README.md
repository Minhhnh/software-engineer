# TT-Inference-Server: Tenstorrent’s LLM Inference Server Overview

- https://github.com/tenstorrent/tt-inference-server

## Overall Purpose

**TT-Inference-Server** is Tenstorrent’s open-source inference server, designed to deploy Large Language Models (LLMs) (and some vision models) on Tenstorrent AI hardware. In essence, it provides a ready-to-run serving stack that integrates Tenstorrent’s custom accelerator libraries with high-level model APIs. The repository contains implementations of various model inference “APIs” (model handlers) which are optimized for Tenstorrent devices, so users can serve models on Tenstorrent’s **Wormhole** accelerators (e.g. **TT-LoudBox**, **TT-QuietBox** desktops or **n150** PCIe cards) with minimal setup. By abstracting the hardware details behind a standard interface (often an OpenAI-compatible REST API), TT-Inference-Server lets developers interact with Tenstorrent-powered models just like they would with a typical GPU-based server, but harnessing Tenstorrent’s efficiency.

## Architecture and Design

TT-Inference-Server’s architecture centers on a combination of Tenstorrent’s low-level libraries and an advanced LLM serving framework:

- **vLLM Integration:** At its core, TT-Inference-Server builds on _vLLM_, a high-performance inference engine for LLMs. It uses vLLM to handle model execution and request batching, thereby supporting features like fast **token streaming** and an OpenAI-like API for chat completions - [dstack.ai](https://dstack.ai/examples/accelerators/tenstorrent/). In fact, Tenstorrent provides a Dockerized “vLLM inference server” as part of this repo, which launches a vLLM-based API service pre-configured for their hardware. This means the server can accept chat/completion requests (via REST endpoints) and generate responses using the loaded model, benefiting from vLLM’s optimized scheduling of parallel requests.
- **TT-Metal and TT-NN Integration:** Under the hood, the server relies on Tenstorrent’s proprietary libraries – **TT-NN** and **TT-Metal** – to run neural network operations on the Tenstorrent chips. TT-NN is a Python/C++ operator library for standard deep learning ops, and TT-Metal (Metalium) is a low-level kernel programming model for the hardware. The inference server’s model implementations replace or augment default operations with TT-NN calls so that heavy compute (matrix multiplies, transformer attention, etc.) executes on Tenstorrent accelerators instead of CPU/GPU. In practice, TT-Inference-Server bridges vLLM with TT-Metal: when vLLM’s engine calls the model layers, those layers leverage TT-NN kernels optimized for Tenstorrent devices. This layered design keeps the high-level workflow (token generation, batching) in Python, while delegating compute-intensive ops to Tenstorrent’s highly parallel cores.
- **Modular Model Handlers:** The repository is structured to support multiple models through a consistent interface. For LLMs, there is a primary module (under `vllm-tt-metal-llama3/`) that contains the model definitions, weight-loading logic, and any custom kernels needed for models like Llama, Qwen, etc. Each supported model is registered with a configuration (model name, HuggingFace repo, required library versions) so the server knows [how to set it up](https://github.com/tenstorrent/tt-inference-server/releases). This modular approach makes it easier to add new models – typically by writing a new config entry or slight code adaptation if the architecture differs. For example, vision models like YOLOv4 are implemented in a separate module [(`tt-metal-yolov4`)](https://github.com/tenstorrent/tt-inference-server?tab=readme-ov-file#cnns) demonstrating the same pattern applied to CNNs.
- **Workflows and Scripts:** A notable design aspect is the concept of [**“workflows,”**](https://github.com/tenstorrent/tt-inference-server/blob/main/docs/workflows_user_guide.md) which are automated sequences for common tasks (setup, running the server, evaluation, etc.). The repo includes Python scripts to orchestrate these workflows. For instance, `run.py` serves as an entry-point that can invoke different workflow modes (starting a server, running evals, building Docker images, etc., depending on CLI args or environment). Similarly, dedicated scripts handle environment setup and container launch. This scripting layer abstracts away many manual steps, enabling one-command setup for supported models. The **Model Readiness Workflow** (documented in the repo) is an example, which automates pulling the correct model files and spinning up a containerized server. Internally, the code is organized with helper modules for things like virtual environment management and logging of each workflow step. Design patterns like this emphasize reproducibility and consistency – important given the variety of models and hardware configurations the server supports.

Overall, the architecture can be seen as a **two-layer system**: the upper layer (vLLM-based server and Python orchestration) handles model-agnostic logic – request handling, token generation loop, etc. – while the lower layer (TT-NN/Metal kernels and model-specific code) handles the model math on the Tenstorrent silicon. These layers communicate through well-defined interfaces, which is why the TT-Inference-Server can deliver a uniform API across many model types.

## Key Integrations with vLLM and Tenstorrent Stack

[**vLLM:**](https://github.com/tenstorrent/vllm) TT-Inference-Server tightly integrates with vLLM, essentially using it as the backbone for running transformer models. Tenstorrent maintains their own fork of vLLM (visible in their open-source listings), which includes a backend for TT-Metal. The inference server uses this customized vLLM to launch an API service (usually on port 7000 by default) that is [compatible with OpenAI’s API format](https://dstack.ai/examples/accelerators/tenstorrent/). When you start TT-Inference-Server, it actually runs a script (e.g. `run_vllm_api_server.py`) that loads the chosen model into vLLM’s engine (on Tenstorrent hardware) and starts the Flask/Uvicorn server to listen for requests. This integration means developers can interact with Tenstorrent-run models using the same protocols and libraries they would use for OpenAI or Open-source API (for example, using an `openai.ChatCompletion.create()` call pointing at this server). The use of vLLM is strategic – it provides high throughput and dynamic batching, which helps maximize the utilization of the Tenstorrent chip by combining multiple queries or stream responses efficiently.

[**TT-NN / TT-Metal:**](https://github.com/tenstorrent/tt-metal) These are the crucial libraries that interface with the actual Tenstorrent **Wormhole** chips (or the older **Grayskull** architecture, if applicable). TT-NN (part of the TT-Metal repository) provides a set of neural network operations optimized in C++ for the hardware, and TT-Metalium provides the lower-level APIs if custom kernels are needed. In TT-Inference-Server, model implementations call TT-NN functions instead of, say, PyTorch or NumPy ops. For example, a transformer’s linear layer might use `ttnn.Linear` or a TT-metal function to perform the matrix multiply on the Tenstorrent device. Similarly, attention mechanisms can leverage TT-Metal kernels. The integration ensures that, once the model weights are loaded onto the card (TT-Metal handles memory management on the device), all forward passes for inference are executed by the Tenstorrent silicon. This is what allows the server to achieve high performance per watt – by using Tenstorrent’s specialized matrix multiplication and sparse computation engines where possible.

It’s worth noting that Tenstorrent’s software stack is still evolving, and the TT-Inference-Server is aligned with that evolution. In the TT-Metal documentation, Tenstorrent notes that running models via vLLM (i.e. through this inference server) might yield different performance than their raw demos, and that _“Blackhole software optimization is under active development”_. This indicates that TT-Inference-Server is part of an active effort to improve the software-hardware synergy. Indeed, as TT-Metal and TT-NN get new releases (they hit v0.59 and v0.60 in mid-2025), the inference server updates its code to stay compatible and leverage improvements (for example, adjusting configs for changes in TT-Metal API).

**Other Tenstorrent Repositories:** TT-Inference-Server operates alongside several other open-source components in Tenstorrent’s ecosystem:

- _Tenstorrent’s vLLM fork:_ As mentioned, Tenstorrent has a fork of vLLM with added `tt_metal` backend support. TT-Inference-Server uses this fork via its submodule or wheel, ensuring that vLLM knows how to offload computations to TT-NN. In the model compatibility table, each model entry even references the specific vLLM commit used, which corresponds to a commit in Tenstorrent’s vLLM repo (for example, commit `e2e0002a` is listed for many models, indicating the vLLM version with TT-metal integration).
- _TT-Forge (TT-MLIR Compiler):_ While not directly part of this repository, TT-Forge is Tenstorrent’s MLIR-based compiler that compiles models for Tenstorrent hardware. In the inference stack, TT-Forge could be used behind the scenes (for example, TT-NN might call into TT-Forge-compiled kernels). TT-Inference-Server primarily interacts with TT-NN at runtime, but it benefits from whatever optimizations TT-Forge has done to those kernels. In short, TT-Forge and TT-NN provide the high-performance implementations that TT-Inference-Server simply calls into. (The repository’s documentation and issues occasionally mention ensuring compatibility with _tt-metal_ and _tt-forge_ versions, implying that compiled kernel binaries or IR must match the runtime.)
- _TT-Torch:_ Tenstorrent also provides **TT-Torch**, a fork of PyTorch for training on their hardware. While TT-Torch is used for model training or conversion, TT-Inference-Server doesn’t directly use PyTorch; it focuses on inference via vLLM. However, models developed or fine-tuned with TT-Torch can be served through TT-Inference-Server after being saved in a supported format (e.g., Hugging Face Transformers format). Thus, TT-Inference-Server complements TT-Torch by taking over at inference time.
- _Utilities like TT-SMI:_ Running on Tenstorrent hardware often involves managing device state (setting up drivers, huge pages, checking device status). TT-Inference-Server expects the environment to be prepared with Tenstorrent’s drivers and tools like `tt-smi` (similar to nvidia-smi). Documentation notes that hosts should have **Tenstorrent software installed (drivers, `tt-smi`, hugepages)** before launching the server. While not an integration in code, it’s an integration in practice: the server runs on top of the drivers that TT-SMI interfaces with.

In summary, TT-Inference-Server is the _glue_ that connects Tenstorrent’s hardware-accelerated backend (TT-NN/Metal) with a user-friendly frontend (vLLM’s API server). It plays nicely with Tenstorrent’s other projects by adhering to their APIs and version compatibilities, effectively sitting at the top of the Tenstorrent software stack to present a cohesive inference solution.

## Supported Models and Configuration

One of the strengths of TT-Inference-Server is its roster of supported models, which spans multiple families of LLMs (and even a CNN). Each supported model is integrated with the server’s workflow, meaning there are known working configurations and (when applicable) container images for them. **As of the latest release, supported models include:**

- **Meta LLaMA Family:** Various versions and sizes of LLaMA are supported, including fine-tuned variants. For example, **Llama-3.3 70B Instruct** (a 70-billion parameter model) is marked “✅ ready” for deployment on LoudBox/QuietBox systems, as are smaller models like Llama-3.2 1B, 3B, 11B etc., some of which have “Instruct” (instruction-tuned) and even a **Vision**-enhanced variant (Llama-3.2-11B-Vision, which likely includes image input capability). The numbering (3.1, 3.2, 3.3) appears to correspond to Meta’s internal or Tenstorrent’s internal versions of LLaMA releases and their fine-tunes. Many LLaMA-based models are listed as “ready”, indicating they have been tested and fully supported on current software versions.
- **Qwen Models:** _Qwen_ (齐悟) is a series of large models from Alibaba. TT-Inference-Server includes support for **Qwen 2.5 - 7B and 72B**, including instruct variants. These are marked “preview”, meaning the integration exists but is under active development or testing. Qwen is architecturally similar to other transformer models, so Tenstorrent was able to integrate it using the same pipeline (with some adjustments for model specifics). Having Qwen-7B and 72B helps cater to use-cases where Chinese language or other abilities of Qwen are needed, demonstrating the server’s extensibility beyond just Meta’s LLaMA.
- **DeepSeek & QwQ:** The list also features models like **DeepSeek-R1-Distill-Llama-70B** (which sounds like a distilled or compressed 70B model) and **QwQ-32B**. These are less standard names – possibly community or Tenstorrent-internal models. _QwQ-32B_ might be an in-house model or a variant of Qwen or another base (the naming is unique), provided at 32B parameters. DeepSeek could be a project involving distillation for efficiency. Both are in “preview” status, implying Tenstorrent is experimenting with them on their hardware. By listing them, the repo shows it’s not limited to one company’s models – any transformer that can fit on the hardware can potentially be integrated.
- **Vision/CNN Model:** Beyond LLMs, TT-Inference-Server also demonstrates support for at least one convolutional model: **YOLOv4** for object detection. In the **CNNs** section of the README, YOLOv4 is listed (with input sizes 320x320, etc.) as supported on the n150 hardware (with preview status). This integration is provided to showcase that the server can handle non-NLP models too by utilizing TT-NN’s convolution kernels. The YOLOv4 implementation lives in its own module (`tt-metal-yolov4`) and likely includes a small web API (perhaps a simple image upload to bounding-box JSON service). While LLMs are the main focus of Tenstorrent’s current inference push, having YOLOv4 indicates the server’s architecture is flexible and that Tenstorrent hardware can accelerate vision tasks as well.
- **Model Configuration:** Each model integration is tied to specific versions of Tenstorrent’s software. The README table enumerates the **`tt-metal` library version and the `vLLM` commit** that were used to get that model working. For instance, a model might require TT-Metal `v0.56.0-rc47` and vLLM commit `e2e0002a` to function correctly. These version pairings are important due to ongoing development – as kernels or vLLM APIs change, the models need slight re-testing. TT-Inference-Server tracks this by pinning versions in its configs. When you select a model to run (either via Docker tag or environment variable), the server knows which internal code path and library versions to use. This ensures reproducibility. Notably, the repository maintainers have automated the Docker image build process for all configured models: a **workflow script can iterate over each supported model, building a container with the correct TT-metal and vLLM versions and including the model weights or download step**. This results in published images per model (the `ghcr.io` images tagged with model and version as seen in the table).
- **Adding/Configuring Models:** The supported list is growing, and one can add new models by contributing to the repository. In general, adding a model involves providing the Hugging Face repo ID (if using HF format weights) and writing a small adapter if the architecture has unique aspects. Many models share the same transformer backbone structure (e.g., LLaMA variants, Qwen, etc.), so they can often reuse the existing Llama integration code with minimal changes – for example, adjusting vocabulary size or prompt format. The repository documentation was updated to guide how to extend support: they moved **setup instructions and model descriptions into a central README** (under `vllm-tt-metal-llama3/README.md`) so that all model-specific quirks are documented in one place. This means if you open that doc, you’ll find instructions on how to set up each model and what each model’s status or requirements are. The aim is to make the process of serving a new model clear: choose your model from the list, follow the steps (which might involve running `setup.sh` or pulling a container), and you’re ready to go.

In summary, TT-Inference-Server already supports a rich set of models – from 1B all the way to 70B+ – covering both language and vision, with a roadmap of more (given many are “preview”). The supported models are configured via a registry inside the code, which ties together the model weights, the code path (which kernel implementations to use), and the exact version of Tenstorrent’s libraries needed. This tight configuration control allows the server to reliably bring up even very large models (like 70B) on Tenstorrent hardware with minimal user hassle. Users just need to ensure they have the model weights (downloadable via Hugging Face) and the server takes care of the rest, such as weight loading and splitting across devices if needed (for example, a 70B model might automatically utilize multiple Tenstorrent chips in parallel if available, using tensor parallelism – TP appears in some model notes).

## Implementation Details and Notable Components

Drilling into the repository, several files and components stand out as key to how TT-Inference-Server works:

- **Model Loading Pipeline:** The process of preparing a model for inference involves downloading weights, converting them if necessary, and loading them onto the device. The script `setup.sh` is central to this. It automates environment setup (checking for dependencies and installing them) and downloads the chosen model’s weights from Hugging Face if not already present. Notably, `setup.sh` has been improved to handle _all_ base and instruct models through a unified flow – meaning you can specify which model you want and it will fetch the correct files automatically (using `huggingface_hub` CLI or APIs). It also performs system checks, like ensuring there’s enough disk space and RAM for the model, which is important before attempting a multi-tens-of-gigabyte model download. After weights are downloaded, there may be a **repacking step** for very large models: for example, LLaMA 70B comes as shards that might need merging, or converting to a format (like safetensors) optimal for TT-Metal. The commit logs mention a `setup_host.py` that was adjusted to fix weight repacking for Llama 3.3/3.1 70B models – this likely combines the shards and possibly partitions them for multi-device execution (TP = Tensor Parallelism).

  Once weights are ready, **loading them into the runtime** happens via the model code (in `vllm_tt_metal_llama3` module). Here, classes representing the Transformer layers are defined to use TT-NN ops. For example, a `TTMetalAttention` class might allocate key/value caches in Tenstorrent device memory and use TT-NN’s fused kernels for attention. Similarly, a `TTMetalMLP` class could wrap a linear layer and activation, calling TT-NN GEMM kernels. The model code also maps the state dict (the weight tensors from HuggingFace format) to TT-NN weights. This could involve converting data types (e.g., FP16 or BF16 for faster inference) and reordering dimensions if the Tenstorrent kernels expect different layouts. The complexity of this mapping is hidden from the user; they just see that the model gets loaded. In the logs, there’s mention of **mocking TT-NN for tests** – the codebase includes a `mock_vllm_model.py` which can create a dummy model object in case you want to run the server on a system without Tenstorrent hardware (for CI or dev). This suggests the architecture cleanly separates the model definition so that it can be swapped or faked, a good design for testing.

- **`run_vllm_api_server.py`:** This is the main launcher script that ties everything together. It sets up the model and starts the vLLM server. Key tasks it handles include parsing environment variables (like `HF_MODEL_REPO_ID` to know which model to load), initializing the vLLM engine with Tenstorrent device as the target, and then calling vLLM’s `HTTPServer` to serve requests. One important function added to this script is `handle_code_versions()`, which ensures that the running environment’s TT-metal version matches what the model expects. Since TT-metal (TT-NN) might introduce breaking changes across versions, this function can adjust config or warn if there’s a mismatch, thereby avoiding runtime errors. The existence of this function, added in a recent update, shows how the server deals with evolving dependencies – by programmatically handling compatibility for different **code versions of TT-metal**.

  After this setup, `run_vllm_api_server.py` hands off to vLLM’s asyncio-based server, which listens for HTTP requests (by default, it provides endpoints like `/v1/chat/completions` or `/v1/completions` akin to OpenAI’s API). The **serving pipeline** at that point is: incoming JSON request –> vLLM request handler –> calls into the model for each token step –> model uses TT-NN to get logits –> vLLM returns the generated tokens to the client. The server supports streaming responses, so tokens can be emitted one by one if requested, which vLLM handles efficiently.

- **Configuration and Workflow Files:** The repository has configuration files that map out model and environment specifics:

  - `model_config.py` (referenced in commit logs) likely contains the master dictionary of model info. In a recent update, they pinned TT-metal and vLLM commits in this config for each model. This file is used by automation scripts to build Docker images for each model. For example, a build script can iterate through all models in `model_config.py`, and for each, plug in the specified TT-metal version and vLLM commit to build a container (so that the container has exactly those versions installed). This ensures each Docker image published (see next section) is tightly version-locked for stability.
  - `workflow_types.py` and `workflow_venvs.py`: these modules define different run modes and virtual environment handling. For instance, _DeviceType_ or _WorkflowType_ enums might be in `workflow_types.py` to distinguish running on CPU vs Tenstorrent vs Docker, etc., and _workflow_venvs.py_ might contain logic to create or reuse Python virtual environments for installing required Python packages for certain tasks (like evaluation vs serving).
  - `prompt_generation.py`, `evals/` directory: there is a framework for automated evaluation of models (perhaps using libraries like EleutherAI’s eval harness or custom prompts). The presence of an `evals_meta` and vision evals in the commit notes indicates the team has set up scripts to benchmark the models on standard tasks. This isn’t directly part of serving, but it is useful for verifying model accuracy/performance after integration.
  - `docker-entrypoint.sh`: The entrypoint script for Docker images, which likely activates the environment and then calls `run_vllm_api_server.py` with the appropriate arguments. It could also handle things like translating environment variables (e.g., passing `HF_MODEL_REPO_ID` into the Python script, or mounting volumes for model data).

- **Performance and Optimization Hooks:** Given Tenstorrent’s hardware has different characteristics than a typical GPU, the implementation includes several performance-related configurations:

  - **Max Context and Concurrency:** Different models and hardware combos have different optimal maximum context lengths and concurrent request limits. For example, a small model on a large memory device could handle a longer context or more parallel requests than a huge model on a smaller device. The code accounts for this via maps of _max context length_ and _max concurrency_ per model/device. These settings might be used to automatically cap the `max_tokens` or batch sizes to avoid memory overflow or latency spikes. When the server starts a model, it can configure vLLM’s engine with these limits (vLLM can be told how many concurrent requests to allow and the context window to assume).
  - **Tracing and Profiling:** There is a mention of a `--disable-trace-capture` CLI option for the workflows. This suggests that by default the server might capture execution traces (possibly for debugging or analyzing performance of kernels on the hardware). Disabling trace capture would reduce overhead, giving slightly better performance during normal operation. It’s an example of a tunable that the implementation provides to balance insight vs speed.
  - **Mocking for Development:** As briefly mentioned, the ability to mock TT-NN means developers can run the server on a regular CPU for functionality testing. In that mode, obviously performance is not a concern, but it helps validate that the high-level logic (request parsing, token looping) works even without a Tenstorrent card. The code achieves this by substituting the real model with a dummy one that perhaps uses torch or numpy for outputs. This design decision – to keep hardware specifics abstracted – is a pattern that eases maintenance and extension.
  - **Ongoing Optimizations:** The Tenstorrent team is continuously optimizing the inference pipeline. Integration with their compiler (TT-Forge) could bring future improvements like quantization or better kernel fusion for inference. Already, the presence of the “Blackhole software optimization” note highlights that they are aware of gaps and are actively improving throughput. We can anticipate updates such as support for larger batches, multi-device scaling (the table references tensor parallel = 8 or 32 for some models, implying the ability to shard a model across multiple chips is either in place or planned), and more efficient memory handling for very large contexts. Because TT-Inference-Server is open-source, these optimizations often come as new commits – for example, a hypothetical commit could introduce int8 quantization support for certain models via TT-NN, and the server would gain that feature. All this to say, the implementation is not static; it evolves with both the hardware capabilities and the software improvements (as seen in frequent commit history addressing performance issues or adding features).

- **Important Directories/Files Quick Summary:** To locate key functionality in the repo:

  - `vllm-tt-metal-llama3/src/` – contains the core model code bridging vLLM and TT-metal for transformers (likely includes model definitions, the `run_vllm_api_server.py`, and utility functions for weight loading).
  - `archive/` – contains older approaches (e.g., a standalone `tt-metal-mistral-7b` integration exists as an archive, which was likely an experiment or initial support for the Mistral 7B model before the unified vLLM approach was adopted).
  - `utils/` – common utilities (possibly logging, and the mock model code).
  - `scripts/` – various Python scripts to run different scenarios: e.g., `run_workflow.py` (for local runs), `run_docker_server.py` (for launching in Docker), `build_release_docker_images.py` (to build all images at once), etc., plus `setup.sh` and environment setup helpers.
  - `docs/` – documentation like the **Workflows User Guide**, which explains how to use the provided workflows and Docker images in a step-by-step manner.
  - `tests/` – likely contains test cases for basic functionality of the server and maybe sanity tests for each model (ensuring a simple prompt returns an expected output, for example).

All these pieces work in concert to provide a smooth experience where, for a given model, you run the setup, launch the server, and get a working endpoint accelerated by Tenstorrent silicon.
