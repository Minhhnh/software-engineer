# TT-Inference-Server: Tenstorrent’s LLM Inference Server Overview

- https://github.com/tenstorrent/tt-inference-server

## Overall Purpose

**TT-Inference-Server** is Tenstorrent’s open-source inference server, designed to deploy Large Language Models (LLMs) (and some vision models) on Tenstorrent AI hardware. In essence, it provides a ready-to-run serving stack that integrates Tenstorrent’s custom accelerator libraries with high-level model APIs. The repository contains implementations of various model inference “APIs” (model handlers) which are optimized for Tenstorrent devices, so users can serve models on Tenstorrent’s **Wormhole** accelerators (e.g. **TT-LoudBox**, **TT-QuietBox** desktops or **n150** PCIe cards) with minimal setup. By abstracting the hardware details behind a standard interface (often an OpenAI-compatible REST API), TT-Inference-Server lets developers interact with Tenstorrent-powered models just like they would with a typical GPU-based server, but harnessing Tenstorrent’s efficiency.

## Architecture and Design

TT-Inference-Server’s architecture centers on a combination of Tenstorrent’s low-level libraries and an advanced LLM serving framework:

- **vLLM Integration:** At its core, TT-Inference-Server builds on _vLLM_, a high-performance inference engine for LLMs. It uses vLLM to handle model execution and request batching, thereby supporting features like fast **token streaming** and an OpenAI-like API for chat completions - [dstack.ai](https://dstack.ai/examples/accelerators/tenstorrent/). In fact, Tenstorrent provides a Dockerized “vLLM inference server” as part of this repo, which launches a vLLM-based API service pre-configured for their hardware. This means the server can accept chat/completion requests (via REST endpoints) and generate responses using the loaded model, benefiting from vLLM’s optimized scheduling of parallel requests.
- **TT-Metal and TT-NN Integration:** Under the hood, the server relies on Tenstorrent’s proprietary libraries – **TT-NN** and **TT-Metal** – to run neural network operations on the Tenstorrent chips. TT-NN is a Python/C++ operator library for standard deep learning ops, and TT-Metal (Metalium) is a low-level kernel programming model for the hardware. The inference server’s model implementations replace or augment default operations with TT-NN calls so that heavy compute (matrix multiplies, transformer attention, etc.) executes on Tenstorrent accelerators instead of CPU/GPU. In practice, TT-Inference-Server bridges vLLM with TT-Metal: when vLLM’s engine calls the model layers, those layers leverage TT-NN kernels optimized for Tenstorrent devices. This layered design keeps the high-level workflow (token generation, batching) in Python, while delegating compute-intensive ops to Tenstorrent’s highly parallel cores.
- **Modular Model Handlers:** The repository is structured to support multiple models through a consistent interface. For LLMs, there is a primary module (under `vllm-tt-metal-llama3/`) that contains the model definitions, weight-loading logic, and any custom kernels needed for models like Llama, Qwen, etc. Each supported model is registered with a configuration (model name, HuggingFace repo, required library versions) so the server knows [how to set it up](https://github.com/tenstorrent/tt-inference-server/releases). This modular approach makes it easier to add new models – typically by writing a new config entry or slight code adaptation if the architecture differs. For example, vision models like YOLOv4 are implemented in a separate module [(`tt-metal-yolov4`)](https://github.com/tenstorrent/tt-inference-server?tab=readme-ov-file#cnns) demonstrating the same pattern applied to CNNs.
- **Workflows and Scripts:** A notable design aspect is the concept of [**“workflows,”**](https://github.com/tenstorrent/tt-inference-server/blob/main/docs/workflows_user_guide.md) which are automated sequences for common tasks (setup, running the server, evaluation, etc.). The repo includes Python scripts to orchestrate these workflows. For instance, `run.py` serves as an entry-point that can invoke different workflow modes (starting a server, running evals, building Docker images, etc., depending on CLI args or environment). Similarly, dedicated scripts handle environment setup and container launch. This scripting layer abstracts away many manual steps, enabling one-command setup for supported models. The **Model Readiness Workflow** (documented in the repo) is an example, which automates pulling the correct model files and spinning up a containerized server. Internally, the code is organized with helper modules for things like virtual environment management and logging of each workflow step. Design patterns like this emphasize reproducibility and consistency – important given the variety of models and hardware configurations the server supports.

Overall, the architecture can be seen as a **two-layer system**: the upper layer (vLLM-based server and Python orchestration) handles model-agnostic logic – request handling, token generation loop, etc. – while the lower layer (TT-NN/Metal kernels and model-specific code) handles the model math on the Tenstorrent silicon. These layers communicate through well-defined interfaces, which is why the TT-Inference-Server can deliver a uniform API across many model types.

## Key Integrations with vLLM and Tenstorrent Stack

[**vLLM:**](https://github.com/tenstorrent/vllm) TT-Inference-Server tightly integrates with vLLM, essentially using it as the backbone for running transformer models. Tenstorrent maintains their own fork of vLLM (visible in their open-source listings), which includes a backend for TT-Metal. The inference server uses this customized vLLM to launch an API service (usually on port 7000 by default) that is [compatible with OpenAI’s API format](https://dstack.ai/examples/accelerators/tenstorrent/). When you start TT-Inference-Server, it actually runs a script (e.g. `run_vllm_api_server.py`) that loads the chosen model into vLLM’s engine (on Tenstorrent hardware) and starts the Flask/Uvicorn server to listen for requests. This integration means developers can interact with Tenstorrent-run models using the same protocols and libraries they would use for OpenAI or Open-source API (for example, using an `openai.ChatCompletion.create()` call pointing at this server). The use of vLLM is strategic – it provides high throughput and dynamic batching, which helps maximize the utilization of the Tenstorrent chip by combining multiple queries or stream responses efficiently.

[**TT-NN / TT-Metal:**](https://github.com/tenstorrent/tt-metal) These are the crucial libraries that interface with the actual Tenstorrent **Wormhole** chips (or the older **Grayskull** architecture, if applicable). TT-NN (part of the TT-Metal repository) provides a set of neural network operations optimized in C++ for the hardware, and TT-Metalium provides the lower-level APIs if custom kernels are needed. In TT-Inference-Server, model implementations call TT-NN functions instead of, say, PyTorch or NumPy ops. For example, a transformer’s linear layer might use `ttnn.Linear` or a TT-metal function to perform the matrix multiply on the Tenstorrent device. Similarly, attention mechanisms can leverage TT-Metal kernels. The integration ensures that, once the model weights are loaded onto the card (TT-Metal handles memory management on the device), all forward passes for inference are executed by the Tenstorrent silicon. This is what allows the server to achieve high performance per watt – by using Tenstorrent’s specialized matrix multiplication and sparse computation engines where possible.

It’s worth noting that Tenstorrent’s software stack is still evolving, and the TT-Inference-Server is aligned with that evolution. In the TT-Metal documentation, Tenstorrent notes that running models via vLLM (i.e. through this inference server) might yield different performance than their raw demos, and that _“Blackhole software optimization is under active development”_. This indicates that TT-Inference-Server is part of an active effort to improve the software-hardware synergy. Indeed, as TT-Metal and TT-NN get new releases (they hit v0.59 and v0.60 in mid-2025), the inference server updates its code to stay compatible and leverage improvements (for example, adjusting configs for changes in TT-Metal API).

**Other Tenstorrent Repositories:** TT-Inference-Server operates alongside several other open-source components in Tenstorrent’s ecosystem:

- _Tenstorrent’s vLLM fork:_ As mentioned, Tenstorrent has a fork of vLLM with added `tt_metal` backend support. TT-Inference-Server uses this fork via its submodule or wheel, ensuring that vLLM knows how to offload computations to TT-NN. In the model compatibility table, each model entry even references the specific vLLM commit used, which corresponds to a commit in Tenstorrent’s vLLM repo (for example, commit `e2e0002a` is listed for many models, indicating the vLLM version with TT-metal integration).
- _TT-Forge (TT-MLIR Compiler):_ While not directly part of this repository, TT-Forge is Tenstorrent’s MLIR-based compiler that compiles models for Tenstorrent hardware. In the inference stack, TT-Forge could be used behind the scenes (for example, TT-NN might call into TT-Forge-compiled kernels). TT-Inference-Server primarily interacts with TT-NN at runtime, but it benefits from whatever optimizations TT-Forge has done to those kernels. In short, TT-Forge and TT-NN provide the high-performance implementations that TT-Inference-Server simply calls into. (The repository’s documentation and issues occasionally mention ensuring compatibility with _tt-metal_ and _tt-forge_ versions, implying that compiled kernel binaries or IR must match the runtime.)
- _TT-Torch:_ Tenstorrent also provides **TT-Torch**, a fork of PyTorch for training on their hardware. While TT-Torch is used for model training or conversion, TT-Inference-Server doesn’t directly use PyTorch; it focuses on inference via vLLM. However, models developed or fine-tuned with TT-Torch can be served through TT-Inference-Server after being saved in a supported format (e.g., Hugging Face Transformers format). Thus, TT-Inference-Server complements TT-Torch by taking over at inference time.
- _Utilities like TT-SMI:_ Running on Tenstorrent hardware often involves managing device state (setting up drivers, huge pages, checking device status). TT-Inference-Server expects the environment to be prepared with Tenstorrent’s drivers and tools like `tt-smi` (similar to nvidia-smi). Documentation notes that hosts should have **Tenstorrent software installed (drivers, `tt-smi`, hugepages)** before launching the server. While not an integration in code, it’s an integration in practice: the server runs on top of the drivers that TT-SMI interfaces with.

In summary, TT-Inference-Server is the _glue_ that connects Tenstorrent’s hardware-accelerated backend (TT-NN/Metal) with a user-friendly frontend (vLLM’s API server). It plays nicely with Tenstorrent’s other projects by adhering to their APIs and version compatibilities, effectively sitting at the top of the Tenstorrent software stack to present a cohesive inference solution.
