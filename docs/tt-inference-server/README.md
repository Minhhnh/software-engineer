# TT-Inference-Server: Tenstorrent’s LLM Inference Server Overview

- https://github.com/tenstorrent/tt-inference-server

## Overall Purpose

**TT-Inference-Server** is Tenstorrent’s open-source inference server, designed to deploy Large Language Models (LLMs) (and some vision models) on Tenstorrent AI hardware. In essence, it provides a ready-to-run serving stack that integrates Tenstorrent’s custom accelerator libraries with high-level model APIs. The repository contains implementations of various model inference “APIs” (model handlers) which are optimized for Tenstorrent devices, so users can serve models on Tenstorrent’s **Wormhole** accelerators (e.g. **TT-LoudBox**, **TT-QuietBox** desktops or **n150** PCIe cards) with minimal setup. By abstracting the hardware details behind a standard interface (often an OpenAI-compatible REST API), TT-Inference-Server lets developers interact with Tenstorrent-powered models just like they would with a typical GPU-based server, but harnessing Tenstorrent’s efficiency.

## Architecture and Design

TT-Inference-Server’s architecture centers on a combination of Tenstorrent’s low-level libraries and an advanced LLM serving framework:

- **vLLM Integration:** At its core, TT-Inference-Server builds on _vLLM_, a high-performance inference engine for LLMs. It uses vLLM to handle model execution and request batching, thereby supporting features like fast **token streaming** and an OpenAI-like API for chat completions - [dstack.ai](https://dstack.ai/examples/accelerators/tenstorrent/). In fact, Tenstorrent provides a Dockerized “vLLM inference server” as part of this repo, which launches a vLLM-based API service pre-configured for their hardware. This means the server can accept chat/completion requests (via REST endpoints) and generate responses using the loaded model, benefiting from vLLM’s optimized scheduling of parallel requests.
- **TT-Metal and TT-NN Integration:** Under the hood, the server relies on Tenstorrent’s proprietary libraries – **TT-NN** and **TT-Metal** – to run neural network operations on the Tenstorrent chips. TT-NN is a Python/C++ operator library for standard deep learning ops, and TT-Metal (Metalium) is a low-level kernel programming model for the hardware. The inference server’s model implementations replace or augment default operations with TT-NN calls so that heavy compute (matrix multiplies, transformer attention, etc.) executes on Tenstorrent accelerators instead of CPU/GPU. In practice, TT-Inference-Server bridges vLLM with TT-Metal: when vLLM’s engine calls the model layers, those layers leverage TT-NN kernels optimized for Tenstorrent devices. This layered design keeps the high-level workflow (token generation, batching) in Python, while delegating compute-intensive ops to Tenstorrent’s highly parallel cores.
- **Modular Model Handlers:** The repository is structured to support multiple models through a consistent interface. For LLMs, there is a primary module (under `vllm-tt-metal-llama3/`) that contains the model definitions, weight-loading logic, and any custom kernels needed for models like Llama, Qwen, etc. Each supported model is registered with a configuration (model name, HuggingFace repo, required library versions) so the server knows [how to set it up](https://github.com/tenstorrent/tt-inference-server/releases). This modular approach makes it easier to add new models – typically by writing a new config entry or slight code adaptation if the architecture differs. For example, vision models like YOLOv4 are implemented in a separate module [(`tt-metal-yolov4`)](https://github.com/tenstorrent/tt-inference-server?tab=readme-ov-file#cnns) demonstrating the same pattern applied to CNNs.
- **Workflows and Scripts:** A notable design aspect is the concept of [**“workflows,”**](https://github.com/tenstorrent/tt-inference-server/blob/main/docs/workflows_user_guide.md) which are automated sequences for common tasks (setup, running the server, evaluation, etc.). The repo includes Python scripts to orchestrate these workflows. For instance, `run.py` serves as an entry-point that can invoke different workflow modes (starting a server, running evals, building Docker images, etc., depending on CLI args or environment). Similarly, dedicated scripts handle environment setup and container launch. This scripting layer abstracts away many manual steps, enabling one-command setup for supported models. The **Model Readiness Workflow** (documented in the repo) is an example, which automates pulling the correct model files and spinning up a containerized server. Internally, the code is organized with helper modules for things like virtual environment management and logging of each workflow step. Design patterns like this emphasize reproducibility and consistency – important given the variety of models and hardware configurations the server supports.

Overall, the architecture can be seen as a **two-layer system**: the upper layer (vLLM-based server and Python orchestration) handles model-agnostic logic – request handling, token generation loop, etc. – while the lower layer (TT-NN/Metal kernels and model-specific code) handles the model math on the Tenstorrent silicon. These layers communicate through well-defined interfaces, which is why the TT-Inference-Server can deliver a uniform API across many model types.

## Key Integrations with vLLM and Tenstorrent Stack

[**vLLM:**](https://github.com/tenstorrent/vllm) TT-Inference-Server tightly integrates with vLLM, essentially using it as the backbone for running transformer models. Tenstorrent maintains their own fork of vLLM (visible in their open-source listings), which includes a backend for TT-Metal. The inference server uses this customized vLLM to launch an API service (usually on port 7000 by default) that is [compatible with OpenAI’s API format](https://dstack.ai/examples/accelerators/tenstorrent/). When you start TT-Inference-Server, it actually runs a script (e.g. `run_vllm_api_server.py`) that loads the chosen model into vLLM’s engine (on Tenstorrent hardware) and starts the Flask/Uvicorn server to listen for requests. This integration means developers can interact with Tenstorrent-run models using the same protocols and libraries they would use for OpenAI or Open-source API (for example, using an `openai.ChatCompletion.create()` call pointing at this server). The use of vLLM is strategic – it provides high throughput and dynamic batching, which helps maximize the utilization of the Tenstorrent chip by combining multiple queries or stream responses efficiently.

[**TT-NN / TT-Metal:**](https://github.com/tenstorrent/tt-metal) These are the crucial libraries that interface with the actual Tenstorrent **Wormhole** chips (or the older **Grayskull** architecture, if applicable). TT-NN (part of the TT-Metal repository) provides a set of neural network operations optimized in C++ for the hardware, and TT-Metalium provides the lower-level APIs if custom kernels are needed. In TT-Inference-Server, model implementations call TT-NN functions instead of, say, PyTorch or NumPy ops. For example, a transformer’s linear layer might use `ttnn.Linear` or a TT-metal function to perform the matrix multiply on the Tenstorrent device. Similarly, attention mechanisms can leverage TT-Metal kernels. The integration ensures that, once the model weights are loaded onto the card (TT-Metal handles memory management on the device), all forward passes for inference are executed by the Tenstorrent silicon. This is what allows the server to achieve high performance per watt – by using Tenstorrent’s specialized matrix multiplication and sparse computation engines where possible.

It’s worth noting that Tenstorrent’s software stack is still evolving, and the TT-Inference-Server is aligned with that evolution. In the TT-Metal documentation, Tenstorrent notes that running models via vLLM (i.e. through this inference server) might yield different performance than their raw demos, and that _“Blackhole software optimization is under active development”_. This indicates that TT-Inference-Server is part of an active effort to improve the software-hardware synergy. Indeed, as TT-Metal and TT-NN get new releases (they hit v0.59 and v0.60 in mid-2025), the inference server updates its code to stay compatible and leverage improvements (for example, adjusting configs for changes in TT-Metal API).

**Other Tenstorrent Repositories:** TT-Inference-Server operates alongside several other open-source components in Tenstorrent’s ecosystem:

- _Tenstorrent’s vLLM fork:_ As mentioned, Tenstorrent has a fork of vLLM with added `tt_metal` backend support. TT-Inference-Server uses this fork via its submodule or wheel, ensuring that vLLM knows how to offload computations to TT-NN. In the model compatibility table, each model entry even references the specific vLLM commit used, which corresponds to a commit in Tenstorrent’s vLLM repo (for example, commit `e2e0002a` is listed for many models, indicating the vLLM version with TT-metal integration).
- _TT-Forge (TT-MLIR Compiler):_ While not directly part of this repository, TT-Forge is Tenstorrent’s MLIR-based compiler that compiles models for Tenstorrent hardware. In the inference stack, TT-Forge could be used behind the scenes (for example, TT-NN might call into TT-Forge-compiled kernels). TT-Inference-Server primarily interacts with TT-NN at runtime, but it benefits from whatever optimizations TT-Forge has done to those kernels. In short, TT-Forge and TT-NN provide the high-performance implementations that TT-Inference-Server simply calls into. (The repository’s documentation and issues occasionally mention ensuring compatibility with _tt-metal_ and _tt-forge_ versions, implying that compiled kernel binaries or IR must match the runtime.)
- _TT-Torch:_ Tenstorrent also provides **TT-Torch**, a fork of PyTorch for training on their hardware. While TT-Torch is used for model training or conversion, TT-Inference-Server doesn’t directly use PyTorch; it focuses on inference via vLLM. However, models developed or fine-tuned with TT-Torch can be served through TT-Inference-Server after being saved in a supported format (e.g., Hugging Face Transformers format). Thus, TT-Inference-Server complements TT-Torch by taking over at inference time.
- _Utilities like TT-SMI:_ Running on Tenstorrent hardware often involves managing device state (setting up drivers, huge pages, checking device status). TT-Inference-Server expects the environment to be prepared with Tenstorrent’s drivers and tools like `tt-smi` (similar to nvidia-smi). Documentation notes that hosts should have **Tenstorrent software installed (drivers, `tt-smi`, hugepages)** before launching the server. While not an integration in code, it’s an integration in practice: the server runs on top of the drivers that TT-SMI interfaces with.

In summary, TT-Inference-Server is the _glue_ that connects Tenstorrent’s hardware-accelerated backend (TT-NN/Metal) with a user-friendly frontend (vLLM’s API server). It plays nicely with Tenstorrent’s other projects by adhering to their APIs and version compatibilities, effectively sitting at the top of the Tenstorrent software stack to present a cohesive inference solution.

## Supported Models and Configuration

One of the strengths of TT-Inference-Server is its roster of supported models, which spans multiple families of LLMs (and even a CNN). Each supported model is integrated with the server’s workflow, meaning there are known working configurations and (when applicable) container images for them. **As of the latest release, supported models include:**

- **Meta LLaMA Family:** Various versions and sizes of LLaMA are supported, including fine-tuned variants. For example, **Llama-3.3 70B Instruct** (a 70-billion parameter model) is marked “✅ ready” for deployment on LoudBox/QuietBox systems, as are smaller models like Llama-3.2 1B, 3B, 11B etc., some of which have “Instruct” (instruction-tuned) and even a **Vision**-enhanced variant (Llama-3.2-11B-Vision, which likely includes image input capability). The numbering (3.1, 3.2, 3.3) appears to correspond to Meta’s internal or Tenstorrent’s internal versions of LLaMA releases and their fine-tunes. Many LLaMA-based models are listed as “ready”, indicating they have been tested and fully supported on current software versions.
- **Qwen Models:** _Qwen_ (齐悟) is a series of large models from Alibaba. TT-Inference-Server includes support for **Qwen 2.5 - 7B and 72B**, including instruct variants. These are marked “preview”, meaning the integration exists but is under active development or testing. Qwen is architecturally similar to other transformer models, so Tenstorrent was able to integrate it using the same pipeline (with some adjustments for model specifics). Having Qwen-7B and 72B helps cater to use-cases where Chinese language or other abilities of Qwen are needed, demonstrating the server’s extensibility beyond just Meta’s LLaMA.
- **DeepSeek & QwQ:** The list also features models like **DeepSeek-R1-Distill-Llama-70B** (which sounds like a distilled or compressed 70B model) and **QwQ-32B**. These are less standard names – possibly community or Tenstorrent-internal models. _QwQ-32B_ might be an in-house model or a variant of Qwen or another base (the naming is unique), provided at 32B parameters. DeepSeek could be a project involving distillation for efficiency. Both are in “preview” status, implying Tenstorrent is experimenting with them on their hardware. By listing them, the repo shows it’s not limited to one company’s models – any transformer that can fit on the hardware can potentially be integrated.
- **Vision/CNN Model:** Beyond LLMs, TT-Inference-Server also demonstrates support for at least one convolutional model: **YOLOv4** for object detection. In the **CNNs** section of the README, YOLOv4 is listed (with input sizes 320x320, etc.) as supported on the n150 hardware (with preview status). This integration is provided to showcase that the server can handle non-NLP models too by utilizing TT-NN’s convolution kernels. The YOLOv4 implementation lives in its own module (`tt-metal-yolov4`) and likely includes a small web API (perhaps a simple image upload to bounding-box JSON service). While LLMs are the main focus of Tenstorrent’s current inference push, having YOLOv4 indicates the server’s architecture is flexible and that Tenstorrent hardware can accelerate vision tasks as well.
- **Model Configuration:** Each model integration is tied to specific versions of Tenstorrent’s software. The README table enumerates the **`tt-metal` library version and the `vLLM` commit** that were used to get that model working. For instance, a model might require TT-Metal `v0.56.0-rc47` and vLLM commit `e2e0002a` to function correctly. These version pairings are important due to ongoing development – as kernels or vLLM APIs change, the models need slight re-testing. TT-Inference-Server tracks this by pinning versions in its configs. When you select a model to run (either via Docker tag or environment variable), the server knows which internal code path and library versions to use. This ensures reproducibility. Notably, the repository maintainers have automated the Docker image build process for all configured models: a **workflow script can iterate over each supported model, building a container with the correct TT-metal and vLLM versions and including the model weights or download step**. This results in published images per model (the `ghcr.io` images tagged with model and version as seen in the table).
- **Adding/Configuring Models:** The supported list is growing, and one can add new models by contributing to the repository. In general, adding a model involves providing the Hugging Face repo ID (if using HF format weights) and writing a small adapter if the architecture has unique aspects. Many models share the same transformer backbone structure (e.g., LLaMA variants, Qwen, etc.), so they can often reuse the existing Llama integration code with minimal changes – for example, adjusting vocabulary size or prompt format. The repository documentation was updated to guide how to extend support: they moved **setup instructions and model descriptions into a central README** (under `vllm-tt-metal-llama3/README.md`) so that all model-specific quirks are documented in one place. This means if you open that doc, you’ll find instructions on how to set up each model and what each model’s status or requirements are. The aim is to make the process of serving a new model clear: choose your model from the list, follow the steps (which might involve running `setup.sh` or pulling a container), and you’re ready to go.

In summary, TT-Inference-Server already supports a rich set of models – from 1B all the way to 70B+ – covering both language and vision, with a roadmap of more (given many are “preview”). The supported models are configured via a registry inside the code, which ties together the model weights, the code path (which kernel implementations to use), and the exact version of Tenstorrent’s libraries needed. This tight configuration control allows the server to reliably bring up even very large models (like 70B) on Tenstorrent hardware with minimal user hassle. Users just need to ensure they have the model weights (downloadable via Hugging Face) and the server takes care of the rest, such as weight loading and splitting across devices if needed (for example, a 70B model might automatically utilize multiple Tenstorrent chips in parallel if available, using tensor parallelism – TP appears in some model notes).

## Implementation Details and Notable Components

Drilling into the repository, several files and components stand out as key to how TT-Inference-Server works:

- **Model Loading Pipeline:** The process of preparing a model for inference involves downloading weights, converting them if necessary, and loading them onto the device. The script `setup.sh` is central to this. It automates environment setup (checking for dependencies and installing them) and downloads the chosen model’s weights from Hugging Face if not already present. Notably, `setup.sh` has been improved to handle _all_ base and instruct models through a unified flow – meaning you can specify which model you want and it will fetch the correct files automatically (using `huggingface_hub` CLI or APIs). It also performs system checks, like ensuring there’s enough disk space and RAM for the model, which is important before attempting a multi-tens-of-gigabyte model download. After weights are downloaded, there may be a **repacking step** for very large models: for example, LLaMA 70B comes as shards that might need merging, or converting to a format (like safetensors) optimal for TT-Metal. The commit logs mention a `setup_host.py` that was adjusted to fix weight repacking for Llama 3.3/3.1 70B models – this likely combines the shards and possibly partitions them for multi-device execution (TP = Tensor Parallelism).

  Once weights are ready, **loading them into the runtime** happens via the model code (in `vllm_tt_metal_llama3` module). Here, classes representing the Transformer layers are defined to use TT-NN ops. For example, a `TTMetalAttention` class might allocate key/value caches in Tenstorrent device memory and use TT-NN’s fused kernels for attention. Similarly, a `TTMetalMLP` class could wrap a linear layer and activation, calling TT-NN GEMM kernels. The model code also maps the state dict (the weight tensors from HuggingFace format) to TT-NN weights. This could involve converting data types (e.g., FP16 or BF16 for faster inference) and reordering dimensions if the Tenstorrent kernels expect different layouts. The complexity of this mapping is hidden from the user; they just see that the model gets loaded. In the logs, there’s mention of **mocking TT-NN for tests** – the codebase includes a `mock_vllm_model.py` which can create a dummy model object in case you want to run the server on a system without Tenstorrent hardware (for CI or dev). This suggests the architecture cleanly separates the model definition so that it can be swapped or faked, a good design for testing.

- **`run_vllm_api_server.py`:** This is the main launcher script that ties everything together. It sets up the model and starts the vLLM server. Key tasks it handles include parsing environment variables (like `HF_MODEL_REPO_ID` to know which model to load), initializing the vLLM engine with Tenstorrent device as the target, and then calling vLLM’s `HTTPServer` to serve requests. One important function added to this script is `handle_code_versions()`, which ensures that the running environment’s TT-metal version matches what the model expects. Since TT-metal (TT-NN) might introduce breaking changes across versions, this function can adjust config or warn if there’s a mismatch, thereby avoiding runtime errors. The existence of this function, added in a recent update, shows how the server deals with evolving dependencies – by programmatically handling compatibility for different **code versions of TT-metal**.

  After this setup, `run_vllm_api_server.py` hands off to vLLM’s asyncio-based server, which listens for HTTP requests (by default, it provides endpoints like `/v1/chat/completions` or `/v1/completions` akin to OpenAI’s API). The **serving pipeline** at that point is: incoming JSON request –> vLLM request handler –> calls into the model for each token step –> model uses TT-NN to get logits –> vLLM returns the generated tokens to the client. The server supports streaming responses, so tokens can be emitted one by one if requested, which vLLM handles efficiently.

- **Configuration and Workflow Files:** The repository has configuration files that map out model and environment specifics:

  - `model_config.py` (referenced in commit logs) likely contains the master dictionary of model info. In a recent update, they pinned TT-metal and vLLM commits in this config for each model. This file is used by automation scripts to build Docker images for each model. For example, a build script can iterate through all models in `model_config.py`, and for each, plug in the specified TT-metal version and vLLM commit to build a container (so that the container has exactly those versions installed). This ensures each Docker image published (see next section) is tightly version-locked for stability.
  - `workflow_types.py` and `workflow_venvs.py`: these modules define different run modes and virtual environment handling. For instance, _DeviceType_ or _WorkflowType_ enums might be in `workflow_types.py` to distinguish running on CPU vs Tenstorrent vs Docker, etc., and _workflow_venvs.py_ might contain logic to create or reuse Python virtual environments for installing required Python packages for certain tasks (like evaluation vs serving).
  - `prompt_generation.py`, `evals/` directory: there is a framework for automated evaluation of models (perhaps using libraries like EleutherAI’s eval harness or custom prompts). The presence of an `evals_meta` and vision evals in the commit notes indicates the team has set up scripts to benchmark the models on standard tasks. This isn’t directly part of serving, but it is useful for verifying model accuracy/performance after integration.
  - `docker-entrypoint.sh`: The entrypoint script for Docker images, which likely activates the environment and then calls `run_vllm_api_server.py` with the appropriate arguments. It could also handle things like translating environment variables (e.g., passing `HF_MODEL_REPO_ID` into the Python script, or mounting volumes for model data).

- **Performance and Optimization Hooks:** Given Tenstorrent’s hardware has different characteristics than a typical GPU, the implementation includes several performance-related configurations:

  - **Max Context and Concurrency:** Different models and hardware combos have different optimal maximum context lengths and concurrent request limits. For example, a small model on a large memory device could handle a longer context or more parallel requests than a huge model on a smaller device. The code accounts for this via maps of _max context length_ and _max concurrency_ per model/device. These settings might be used to automatically cap the `max_tokens` or batch sizes to avoid memory overflow or latency spikes. When the server starts a model, it can configure vLLM’s engine with these limits (vLLM can be told how many concurrent requests to allow and the context window to assume).
  - **Tracing and Profiling:** There is a mention of a `--disable-trace-capture` CLI option for the workflows. This suggests that by default the server might capture execution traces (possibly for debugging or analyzing performance of kernels on the hardware). Disabling trace capture would reduce overhead, giving slightly better performance during normal operation. It’s an example of a tunable that the implementation provides to balance insight vs speed.
  - **Mocking for Development:** As briefly mentioned, the ability to mock TT-NN means developers can run the server on a regular CPU for functionality testing. In that mode, obviously performance is not a concern, but it helps validate that the high-level logic (request parsing, token looping) works even without a Tenstorrent card. The code achieves this by substituting the real model with a dummy one that perhaps uses torch or numpy for outputs. This design decision – to keep hardware specifics abstracted – is a pattern that eases maintenance and extension.
  - **Ongoing Optimizations:** The Tenstorrent team is continuously optimizing the inference pipeline. Integration with their compiler (TT-Forge) could bring future improvements like quantization or better kernel fusion for inference. Already, the presence of the “Blackhole software optimization” note highlights that they are aware of gaps and are actively improving throughput. We can anticipate updates such as support for larger batches, multi-device scaling (the table references tensor parallel = 8 or 32 for some models, implying the ability to shard a model across multiple chips is either in place or planned), and more efficient memory handling for very large contexts. Because TT-Inference-Server is open-source, these optimizations often come as new commits – for example, a hypothetical commit could introduce int8 quantization support for certain models via TT-NN, and the server would gain that feature. All this to say, the implementation is not static; it evolves with both the hardware capabilities and the software improvements (as seen in frequent commit history addressing performance issues or adding features).

- **Important Directories/Files Quick Summary:** To locate key functionality in the repo:

  - `vllm-tt-metal-llama3/src/` – contains the core model code bridging vLLM and TT-metal for transformers (likely includes model definitions, the `run_vllm_api_server.py`, and utility functions for weight loading).
  - `archive/` – contains older approaches (e.g., a standalone `tt-metal-mistral-7b` integration exists as an archive, which was likely an experiment or initial support for the Mistral 7B model before the unified vLLM approach was adopted).
  - `utils/` – common utilities (possibly logging, and the mock model code).
  - `scripts/` – various Python scripts to run different scenarios: e.g., `run_workflow.py` (for local runs), `run_docker_server.py` (for launching in Docker), `build_release_docker_images.py` (to build all images at once), etc., plus `setup.sh` and environment setup helpers.
  - `docs/` – documentation like the **Workflows User Guide**, which explains how to use the provided workflows and Docker images in a step-by-step manner.
  - `tests/` – likely contains test cases for basic functionality of the server and maybe sanity tests for each model (ensuring a simple prompt returns an expected output, for example).

All these pieces work in concert to provide a smooth experience where, for a given model, you run the setup, launch the server, and get a working endpoint accelerated by Tenstorrent silicon.

## Deployment and Usage

TT-Inference-Server is designed to be deployed in a couple of ways, primarily focusing on **Docker-based deployment** for ease of use, but also supporting bare-metal operation for developers on Tenstorrent systems.

- **Pre-built Docker Images:** Tenstorrent provides ready-to-use Docker images for each supported model and version. These images are hosted on GitHub’s Container Registry (ghcr.io) under the `tenstorrent/tt-inference-server` namespace. In the model table, a **“Docker Image”** tag is listed for every model, corresponding to a specific image version. For example, an image tag might be `ghcr.io/tenstorrent/tt-inference-server/vllm-tt-metal-src-release-ubuntu-20.04-amd64:0.0.4-v0.56.0-rc47-e2e0002ac7dc` (this was one used for Llama-3.2-1B in a guide). The tag encodes the TT-Inference-Server version (0.0.4), the TT-Metal version (v0.56.0-rc47), and the vLLM commit (e2e0002ac7dc) in a single string – ensuring you pull the exact environment tested for that model.

  Using these images is straightforward: you supply two key environment variables to the container – `HF_TOKEN` (your Hugging Face token, if the model is behind authentication or to avoid rate limits) and `HF_MODEL_REPO_ID` (the HuggingFace model identifier, e.g. `"meta-llama/Llama-3.2-1B-Instruct"`) – and then run the container. The container’s entrypoint will take care of downloading the model weights (if not already cached), setting up the runtime, and launching the server. For instance, a docker run command (or a `dstack` service as shown in an example) will ultimately execute something like:

  ```bash
  docker run -p 7000:7000 -e HF_TOKEN=<your_token> -e HF_MODEL_REPO_ID=meta-llama/Llama-3.2-1B-Instruct \
    ghcr.io/tenstorrent/tt-inference-server/vllm-tt-metal-src-release-ubuntu-20.04-amd64:0.0.4-...
  ```

  Internally this triggers `huggingface-cli download` for that model and then `python run_vllm_api_server.py`. Within seconds (depending on model size and download speed), the model will be loaded onto the Tenstorrent accelerator and the server will be listening on port 7000 (configurable).

  The Tenstorrent inference server speaks a REST API that mirrors the OpenAI format. So to query the model, you would POST to the `/v1/chat/completions` or `/v1/completions` endpoint of the server. In a local deployment, for example, you could run:

  ```bash
  curl http://localhost:7000/v1/chat/completions \
       -H "Content-Type: application/json" \
       -d '{
             "model": "meta-llama/Llama-3.2-1B-Instruct",
             "messages": [ {"role": "user", "content": "What is Deep Learning?"} ],
             "max_tokens": 512,
             "stream": true
           }'
  ```

  and the server would stream back a completion for the prompt. (When using these images via an orchestrator like `dstack` or Kubernetes, the URL might be proxied – but the idea is the same; you get an HTTP endpoint for completions.)

- **Bare-Metal Deployment:** For users who are developing on a system with Tenstorrent cards (such as a dev rig with an n150 card or a Wormhole server), the repository allows running outside of Docker too. In this case, you’d clone the repository on that machine, run `setup.sh` with the appropriate parameters to install dependencies and download the model, and then execute `python run_vllm_api_server.py` (or use `run.py` with arguments) to start the server. The setup script will ensure that **TT-Metal/TT-NN is installed** (either via pip if available or by pointing to the correct commit) and that all Python requirements are satisfied. You’ll need to have done the one-time system prep: installing the Tenstorrent driver and runtime per Tenstorrent’s docs (which includes enabling hugepages, installing the kernel driver, and the `tt-smi` tool). Once that’s in place, running TT-Inference-Server on bare metal is similar to Docker, minus the container – it uses the host’s Python environment. The advantage here is easier debugging and possibly slightly better performance (no container overhead), which is useful for development or if you want to integrate the server tightly with other processes on the machine.

- **Deployment Use-Cases:** TT-Inference-Server can be used in both **interactive** scenarios and **production** scenarios:

  - For testing or development, you might run a Jupyter notebook on the same machine and query the local server to get model outputs, or use it to benchmark model speed on your Tenstorrent card.
  - In a production or demo environment, one could host the server as a long-running Docker container. Because it supports the OpenAI API schema, you can plug it into existing applications or model inference pipelines with minimal changes – just point the API URL to this server instead of OpenAI. This is great for companies or researchers who have Tenstorrent hardware and want to serve an LLM to their internal users or even external via a controlled API.
  - The server currently focuses on single-model hosting (one model per process/container). If you want multiple models simultaneously, you would run multiple containers on the same machine (provided you have enough hardware resources, or split the accelerators among them). Tenstorrent’s roadmap might include a multi-model server in the future, but for now, one model = one server instance, which aligns with vLLM’s design.

- **Scaling and Management:** In terms of scaling up, since vLLM (and by extension TT-Inference-Server) can handle multiple concurrent requests by dynamic batching, a single instance can service many users up to the limits of the hardware. The “max_concurrency_map” in config ensures you don’t overload the device. If more throughput is needed, you can of course deploy more instances on additional hardware (or use multiple Tenstorrent cards: the server can possibly utilize more than one card for a single model via tensor parallel, although the exact command for multi-card usage isn’t explicitly described in the docs we saw – it might be automatic if TT-metal sees multiple devices, or via config).

- **Docker vs. Source:** Most users will prefer the Docker method because it encapsulates all dependencies (especially since TT-metal has specific version requirements and system packages). The Docker images are based on Ubuntu 20.04 (per the tag) and likely include all needed binaries. For advanced users who want to run from source, it’s certainly possible – the repo provides a `pyproject.toml` and `requirements-dev.txt` for pip setup. After installing the right version of `tt-metal` (which might be a pip wheel provided by Tenstorrent or built from source) and the Tenstorrent fork of `vllm`, one can operate directly in a Python environment. This might be done for integration with other Python code (where you don’t want a separate process and instead want to import and call the model in-process). The codebase is flexible enough to allow that, though the primary design is as a server.

- **Deployment Examples:** The repository’s **Model Readiness Workflows User Guide** (linked in the README) likely provides step-by-step examples of deploying each model, including sample commands and expected outputs. Community videos (like Tenstorrent’s YouTube “Running LLMs using TT-Inference-Server”) also demonstrate the two methods: one, using Docker to quickly bring up a model on a **TT-LoudBox** machine; and two, manually setting up on a dev board for more control. These resources confirm that with just a few commands, one can have a powerful model like a 70B LLaMA running on Tenstorrent hardware, ready to answer queries.

In summary, **TT-Inference-Server’s deployment is user-friendly**: pull the image (or code), provide model info, and run. It abstracts away the intricate steps of device configuration, so you don’t need to manually compile kernels or manage device memory – the server and TT-metal runtime handle that internally. This lowers the barrier for organizations to utilize Tenstorrent hardware in their AI inference workflows, by providing a familiar serving interface (mirroring industry-standard APIs) packaged in an easily deployable form.

## Current Maintenance and Activity Status

TT-Inference-Server is an **actively developed project** within Tenstorrent’s open-source ecosystem. Although it’s relatively young (version 0.0.x as of early 2024), it has seen frequent updates and is under continuous maintenance by Tenstorrent engineers. Here are some indicators of its vitality:

- **Regular Releases and Commits:** In the first few months of 2024, the repository went through multiple release candidates (v0.0.3 in Feb 2024, v0.0.4 in Mar 2024, etc.), each adding significant features and fixes. For instance, v0.0.3 focused on improving the setup process and documentation for all supported models, and v0.0.4 introduced automated workflows for model readiness and added new model support (such as vision models and updated LLaMA versions). The commit history (over 200 commits by that time) reflects work on both user-facing improvements and internal refactoring to keep the codebase robust.

- **Responsive to Issues:** The repository has an open issue tracker with dozens of issues, and the maintainers encourage users to file issues if they encounter problems, especially for the models marked “preview”. The README explicitly notes that preview models are under active development and that the team will address reported problems. This kind of statement, along with the presence of several recently merged pull requests, shows that Tenstorrent is actively supporting the users of this repo. Indeed, the primary maintainer (GitHub user `@tstescoTT` and others) has been merging fixes and responding to feedback – for example, adding disk/RAM checks after a user suggestion, or updating the code to be compatible with new TT-Metal releases as they come out.

- **Evolving with Hardware & Software:** As Tenstorrent releases new hardware (like the larger Wormhole **Galaxy** servers) or new software (TT-Metal 0.57, 0.58, etc.), TT-Inference-Server is updated to accommodate them. The model table was expanded to include entries for new model versions (Llama 3.3, etc.) and new hardware configs (Galaxy with more parallelism) as those became available. This suggests a close synergy – likely the same team that works on TT-Metal and model demos also ensures the inference server works for those demos. In June 2025, for example, TT-Metal saw major performance tuning (the “Latest Releases” section in docs shows performance metrics updated in June 2025), and we can expect TT-Inference-Server to incorporate those improvements either automatically or via minor code changes.

- **Community and Open Future:** Tenstorrent has been vocal about their commitment to open source AI, as evidenced by their OpenFuture initiative. TT-Inference-Server being public on GitHub with an Apache-2.0 license means the community can contribute as well. While still niche (the user base is inherently limited to those with Tenstorrent hardware), it has a growing interest. The repository’s stars and forks counts are modest but climbing, indicating that early adopters and partners are engaging. Tenstorrent’s Discord and forums are available for discussion (the TT-Metal readme invites developers to join Discord to shape the future of open source AI). This implies TT-Inference-Server will continue to be maintained not just by Tenstorrent internally but also with input from external developers.

- **Stability vs Preview:** At the time of writing, some models are marked “✅ ready” (which means they have been thoroughly validated on the current stack), whereas many cutting-edge models are “preview”. The balance is tipping towards more models becoming ready as the software matures. For instance, Llama-3.1 and smaller models are already production-ready on TT-Inference-Server, and as the preview ones get more testing (or as TT-metal features expand to fully support them), they will be promoted. The maintainers update the status in the README accordingly, so it’s a live indicator of development progress. The presence of preview models shows the project is not stagnant – it’s willing to include experimental support early, then iterate to reach stability. This agile approach benefits users who want access to new models sooner (even if with caveats), and it showcases Tenstorrent’s development pace.

- **Frequency of Updates:** Judging by release timestamps and repository activity, updates are frequent. For example, between v0.0.3 and v0.0.4 there were roughly 6 weeks, with many issues closed in that span. And even after v0.0.4 (March 2024), we see commits related to newer TT-metal versions (v0.56, v0.59, etc. in the trendshift data) going into the code. By mid-2025, TT-Inference-Server likely has a v0.0.5 or similar, aligning with TT-Metal v0.60. The exact version isn’t listed in the sources, but the continuous alignment with TT-metal’s release cycle (almost monthly minor releases) means TT-Inference-Server gets attention at least every few weeks.

In conclusion, TT-Inference-Server is under **active maintenance** by Tenstorrent, showing healthy development signals. It’s a critical piece of Tenstorrent’s strategy to foster an open, user-friendly ecosystem around their AI accelerators. For our upcoming inference architecture reviews and integration work, this means we can count on TT-Inference-Server to be a stable yet evolving platform – one where we can integrate our own models (provided they’re transformer-based) and where improvements from Tenstorrent (in throughput, compatibility, etc.) will continuously flow in. It will be important for us to track the repository for new releases and updates (e.g., join their Discord or watch the GitHub repo) so that our integration remains compatible. Given Tenstorrent’s openness and responsiveness, we can also propose changes or raise issues as we work with it, and expect collaborative support. In summary, TT-Inference-Server provides a clear window into Tenstorrent’s LLM serving stack: it shows what’s implemented, how it ties into their hardware, and it’s actively growing – a solid foundation for any projects aiming to leverage Tenstorrent’s inference capabilities.

## References

1. **Tenstorrent/tt-inference-server GitHub Repository – README**
   [https://github.com/tenstorrent/tt-inference-server](https://github.com/tenstorrent/tt-inference-server)
   Describes the purpose, architecture, supported models, deployment methods, and current roadmap of TT-Inference-Server.

2. **Tenstorrent/tt-inference-server – Model Table**
   [https://github.com/tenstorrent/tt-inference-server#supported-models](https://github.com/tenstorrent/tt-inference-server#supported-models)
   Lists all supported models, their hardware requirements, TT-metal and vLLM version compatibility, and Docker image tags.

3. **Tenstorrent/tt-inference-server – Commit History**
   [https://github.com/tenstorrent/tt-inference-server/commits/main](https://github.com/tenstorrent/tt-inference-server/commits/main)
   Shows recent updates, bug fixes, feature additions, and maintenance activity frequency.

4. **Tenstorrent/tt-metal GitHub Repository**
   [https://github.com/tenstorrent/tt-metal](https://github.com/tenstorrent/tt-metal)
   Provides the underlying runtime, kernels, and device APIs that TT-Inference-Server uses via TT-NN for hardware acceleration.

5. **Tenstorrent TT-Metal Documentation**
   [https://tenstorrent-metal.readthedocs.io/](https://tenstorrent-metal.readthedocs.io/)
   Details the Metalium programming model, kernel APIs, device setup requirements, and integration considerations for inference.

6. **Tenstorrent vLLM Fork Repository**
   (Private or internal fork not publicly listed, but referenced in TT-Inference-Server commits)
   Contains Tenstorrent-specific extensions enabling vLLM to run on TT-Metal.

7. **dstack.ai Guide – Deploying Tenstorrent Inference Server**
   [https://dstack.ai/](https://dstack.ai/) (example deployment guide using TT-Inference-Server)
   Demonstrates Docker deployment, environment setup, and sample API calls to the server.

8. **Tenstorrent YouTube Channel – Inference Demos and Developer Day Talks**
   [https://www.youtube.com/@Tenstorrent](https://www.youtube.com/@Tenstorrent)
   Includes live demos of TT-Inference-Server running LLaMA-70B and Qwen models on Tenstorrent hardware.

9. **Tenstorrent/tt-inference-server – Workflow Scripts and Model Setup**
   [https://github.com/tenstorrent/tt-inference-server/tree/main/scripts](https://github.com/tenstorrent/tt-inference-server/tree/main/scripts)
   Contains setup.sh, run.py, and workflow automation scripts for model readiness and server startup.

10. **Tenstorrent/tt-inference-server – Issues Tracker**
    [https://github.com/tenstorrent/tt-inference-server/issues](https://github.com/tenstorrent/tt-inference-server/issues)
    Shows active issues, user questions, and team responsiveness, indicating current maintenance status.
