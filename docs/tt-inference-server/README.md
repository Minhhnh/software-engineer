# TT-Inference-Server: Tenstorrent’s LLM Inference Server Overview

- https://github.com/tenstorrent/tt-inference-server

## Overall Purpose

**TT-Inference-Server** is Tenstorrent’s open-source inference server, designed to deploy Large Language Models (LLMs) (and some vision models) on Tenstorrent AI hardware. In essence, it provides a ready-to-run serving stack that integrates Tenstorrent’s custom accelerator libraries with high-level model APIs. The repository contains implementations of various model inference “APIs” (model handlers) which are optimized for Tenstorrent devices, so users can serve models on Tenstorrent’s **Wormhole** accelerators (e.g. **TT-LoudBox**, **TT-QuietBox** desktops or **n150** PCIe cards) with minimal setup. By abstracting the hardware details behind a standard interface (often an OpenAI-compatible REST API), TT-Inference-Server lets developers interact with Tenstorrent-powered models just like they would with a typical GPU-based server, but harnessing Tenstorrent’s efficiency.

## Architecture and Design

TT-Inference-Server’s architecture centers on a combination of Tenstorrent’s low-level libraries and an advanced LLM serving framework:

- **vLLM Integration:** At its core, TT-Inference-Server builds on _vLLM_, a high-performance inference engine for LLMs. It uses vLLM to handle model execution and request batching, thereby supporting features like fast **token streaming** and an OpenAI-like API for chat completions - [dstack.ai](https://dstack.ai/examples/accelerators/tenstorrent/). In fact, Tenstorrent provides a Dockerized “vLLM inference server” as part of this repo, which launches a vLLM-based API service pre-configured for their hardware. This means the server can accept chat/completion requests (via REST endpoints) and generate responses using the loaded model, benefiting from vLLM’s optimized scheduling of parallel requests.
- **TT-Metal and TT-NN Integration:** Under the hood, the server relies on Tenstorrent’s proprietary libraries – **TT-NN** and **TT-Metal** – to run neural network operations on the Tenstorrent chips. TT-NN is a Python/C++ operator library for standard deep learning ops, and TT-Metal (Metalium) is a low-level kernel programming model for the hardware. The inference server’s model implementations replace or augment default operations with TT-NN calls so that heavy compute (matrix multiplies, transformer attention, etc.) executes on Tenstorrent accelerators instead of CPU/GPU. In practice, TT-Inference-Server bridges vLLM with TT-Metal: when vLLM’s engine calls the model layers, those layers leverage TT-NN kernels optimized for Tenstorrent devices. This layered design keeps the high-level workflow (token generation, batching) in Python, while delegating compute-intensive ops to Tenstorrent’s highly parallel cores.
- **Modular Model Handlers:** The repository is structured to support multiple models through a consistent interface. For LLMs, there is a primary module (under `vllm-tt-metal-llama3/`) that contains the model definitions, weight-loading logic, and any custom kernels needed for models like Llama, Qwen, etc. Each supported model is registered with a configuration (model name, HuggingFace repo, required library versions) so the server knows [how to set it up](https://github.com/tenstorrent/tt-inference-server/releases). This modular approach makes it easier to add new models – typically by writing a new config entry or slight code adaptation if the architecture differs. For example, vision models like YOLOv4 are implemented in a separate module [(`tt-metal-yolov4`)](https://github.com/tenstorrent/tt-inference-server?tab=readme-ov-file#cnns) demonstrating the same pattern applied to CNNs.
- **Workflows and Scripts:** A notable design aspect is the concept of [**“workflows,”**](https://github.com/tenstorrent/tt-inference-server/blob/main/docs/workflows_user_guide.md) which are automated sequences for common tasks (setup, running the server, evaluation, etc.). The repo includes Python scripts to orchestrate these workflows. For instance, `run.py` serves as an entry-point that can invoke different workflow modes (starting a server, running evals, building Docker images, etc., depending on CLI args or environment). Similarly, dedicated scripts handle environment setup and container launch. This scripting layer abstracts away many manual steps, enabling one-command setup for supported models. The **Model Readiness Workflow** (documented in the repo) is an example, which automates pulling the correct model files and spinning up a containerized server. Internally, the code is organized with helper modules for things like virtual environment management and logging of each workflow step. Design patterns like this emphasize reproducibility and consistency – important given the variety of models and hardware configurations the server supports.

Overall, the architecture can be seen as a **two-layer system**: the upper layer (vLLM-based server and Python orchestration) handles model-agnostic logic – request handling, token generation loop, etc. – while the lower layer (TT-NN/Metal kernels and model-specific code) handles the model math on the Tenstorrent silicon. These layers communicate through well-defined interfaces, which is why the TT-Inference-Server can deliver a uniform API across many model types.

## Key Integrations with vLLM and Tenstorrent Stack

[**vLLM:**](https://github.com/tenstorrent/vllm) TT-Inference-Server tightly integrates with vLLM, essentially using it as the backbone for running transformer models. Tenstorrent maintains their own fork of vLLM (visible in their open-source listings), which includes a backend for TT-Metal. The inference server uses this customized vLLM to launch an API service (usually on port 7000 by default) that is [compatible with OpenAI’s API format](https://dstack.ai/examples/accelerators/tenstorrent/). When you start TT-Inference-Server, it actually runs a script (e.g. `run_vllm_api_server.py`) that loads the chosen model into vLLM’s engine (on Tenstorrent hardware) and starts the Flask/Uvicorn server to listen for requests. This integration means developers can interact with Tenstorrent-run models using the same protocols and libraries they would use for OpenAI or Open-source API (for example, using an `openai.ChatCompletion.create()` call pointing at this server). The use of vLLM is strategic – it provides high throughput and dynamic batching, which helps maximize the utilization of the Tenstorrent chip by combining multiple queries or stream responses efficiently.

[**TT-NN / TT-Metal:**](https://github.com/tenstorrent/tt-metal) These are the crucial libraries that interface with the actual Tenstorrent **Wormhole** chips (or the older **Grayskull** architecture, if applicable). TT-NN (part of the TT-Metal repository) provides a set of neural network operations optimized in C++ for the hardware, and TT-Metalium provides the lower-level APIs if custom kernels are needed. In TT-Inference-Server, model implementations call TT-NN functions instead of, say, PyTorch or NumPy ops. For example, a transformer’s linear layer might use `ttnn.Linear` or a TT-metal function to perform the matrix multiply on the Tenstorrent device. Similarly, attention mechanisms can leverage TT-Metal kernels. The integration ensures that, once the model weights are loaded onto the card (TT-Metal handles memory management on the device), all forward passes for inference are executed by the Tenstorrent silicon. This is what allows the server to achieve high performance per watt – by using Tenstorrent’s specialized matrix multiplication and sparse computation engines where possible.

It’s worth noting that Tenstorrent’s software stack is still evolving, and the TT-Inference-Server is aligned with that evolution. In the TT-Metal documentation, Tenstorrent notes that running models via vLLM (i.e. through this inference server) might yield different performance than their raw demos, and that _“Blackhole software optimization is under active development”_. This indicates that TT-Inference-Server is part of an active effort to improve the software-hardware synergy. Indeed, as TT-Metal and TT-NN get new releases (they hit v0.59 and v0.60 in mid-2025), the inference server updates its code to stay compatible and leverage improvements (for example, adjusting configs for changes in TT-Metal API).

**Other Tenstorrent Repositories:** TT-Inference-Server operates alongside several other open-source components in Tenstorrent’s ecosystem:

- _Tenstorrent’s vLLM fork:_ As mentioned, Tenstorrent has a fork of vLLM with added `tt_metal` backend support. TT-Inference-Server uses this fork via its submodule or wheel, ensuring that vLLM knows how to offload computations to TT-NN. In the model compatibility table, each model entry even references the specific vLLM commit used, which corresponds to a commit in Tenstorrent’s vLLM repo (for example, commit `e2e0002a` is listed for many models, indicating the vLLM version with TT-metal integration).
- _TT-Forge (TT-MLIR Compiler):_ While not directly part of this repository, TT-Forge is Tenstorrent’s MLIR-based compiler that compiles models for Tenstorrent hardware. In the inference stack, TT-Forge could be used behind the scenes (for example, TT-NN might call into TT-Forge-compiled kernels). TT-Inference-Server primarily interacts with TT-NN at runtime, but it benefits from whatever optimizations TT-Forge has done to those kernels. In short, TT-Forge and TT-NN provide the high-performance implementations that TT-Inference-Server simply calls into. (The repository’s documentation and issues occasionally mention ensuring compatibility with _tt-metal_ and _tt-forge_ versions, implying that compiled kernel binaries or IR must match the runtime.)
- _TT-Torch:_ Tenstorrent also provides **TT-Torch**, a fork of PyTorch for training on their hardware. While TT-Torch is used for model training or conversion, TT-Inference-Server doesn’t directly use PyTorch; it focuses on inference via vLLM. However, models developed or fine-tuned with TT-Torch can be served through TT-Inference-Server after being saved in a supported format (e.g., Hugging Face Transformers format). Thus, TT-Inference-Server complements TT-Torch by taking over at inference time.
- _Utilities like TT-SMI:_ Running on Tenstorrent hardware often involves managing device state (setting up drivers, huge pages, checking device status). TT-Inference-Server expects the environment to be prepared with Tenstorrent’s drivers and tools like `tt-smi` (similar to nvidia-smi). Documentation notes that hosts should have **Tenstorrent software installed (drivers, `tt-smi`, hugepages)** before launching the server. While not an integration in code, it’s an integration in practice: the server runs on top of the drivers that TT-SMI interfaces with.

In summary, TT-Inference-Server is the _glue_ that connects Tenstorrent’s hardware-accelerated backend (TT-NN/Metal) with a user-friendly frontend (vLLM’s API server). It plays nicely with Tenstorrent’s other projects by adhering to their APIs and version compatibilities, effectively sitting at the top of the Tenstorrent software stack to present a cohesive inference solution.

## Supported Models and Configuration

One of the strengths of TT-Inference-Server is its roster of supported models, which spans multiple families of LLMs (and even a CNN). Each supported model is integrated with the server’s workflow, meaning there are known working configurations and (when applicable) container images for them. **As of the latest release, supported models include:**

- **Meta LLaMA Family:** Various versions and sizes of LLaMA are supported, including fine-tuned variants. For example, **Llama-3.3 70B Instruct** (a 70-billion parameter model) is marked “✅ ready” for deployment on LoudBox/QuietBox systems, as are smaller models like Llama-3.2 1B, 3B, 11B etc., some of which have “Instruct” (instruction-tuned) and even a **Vision**-enhanced variant (Llama-3.2-11B-Vision, which likely includes image input capability). The numbering (3.1, 3.2, 3.3) appears to correspond to Meta’s internal or Tenstorrent’s internal versions of LLaMA releases and their fine-tunes. Many LLaMA-based models are listed as “ready”, indicating they have been tested and fully supported on current software versions.
- **Qwen Models:** _Qwen_ (齐悟) is a series of large models from Alibaba. TT-Inference-Server includes support for **Qwen 2.5 - 7B and 72B**, including instruct variants. These are marked “preview”, meaning the integration exists but is under active development or testing. Qwen is architecturally similar to other transformer models, so Tenstorrent was able to integrate it using the same pipeline (with some adjustments for model specifics). Having Qwen-7B and 72B helps cater to use-cases where Chinese language or other abilities of Qwen are needed, demonstrating the server’s extensibility beyond just Meta’s LLaMA.
- **DeepSeek & QwQ:** The list also features models like **DeepSeek-R1-Distill-Llama-70B** (which sounds like a distilled or compressed 70B model) and **QwQ-32B**. These are less standard names – possibly community or Tenstorrent-internal models. _QwQ-32B_ might be an in-house model or a variant of Qwen or another base (the naming is unique), provided at 32B parameters. DeepSeek could be a project involving distillation for efficiency. Both are in “preview” status, implying Tenstorrent is experimenting with them on their hardware. By listing them, the repo shows it’s not limited to one company’s models – any transformer that can fit on the hardware can potentially be integrated.
- **Vision/CNN Model:** Beyond LLMs, TT-Inference-Server also demonstrates support for at least one convolutional model: **YOLOv4** for object detection. In the **CNNs** section of the README, YOLOv4 is listed (with input sizes 320x320, etc.) as supported on the n150 hardware (with preview status). This integration is provided to showcase that the server can handle non-NLP models too by utilizing TT-NN’s convolution kernels. The YOLOv4 implementation lives in its own module (`tt-metal-yolov4`) and likely includes a small web API (perhaps a simple image upload to bounding-box JSON service). While LLMs are the main focus of Tenstorrent’s current inference push, having YOLOv4 indicates the server’s architecture is flexible and that Tenstorrent hardware can accelerate vision tasks as well.
- **Model Configuration:** Each model integration is tied to specific versions of Tenstorrent’s software. The README table enumerates the **`tt-metal` library version and the `vLLM` commit** that were used to get that model working. For instance, a model might require TT-Metal `v0.56.0-rc47` and vLLM commit `e2e0002a` to function correctly. These version pairings are important due to ongoing development – as kernels or vLLM APIs change, the models need slight re-testing. TT-Inference-Server tracks this by pinning versions in its configs. When you select a model to run (either via Docker tag or environment variable), the server knows which internal code path and library versions to use. This ensures reproducibility. Notably, the repository maintainers have automated the Docker image build process for all configured models: a **workflow script can iterate over each supported model, building a container with the correct TT-metal and vLLM versions and including the model weights or download step**. This results in published images per model (the `ghcr.io` images tagged with model and version as seen in the table).
- **Adding/Configuring Models:** The supported list is growing, and one can add new models by contributing to the repository. In general, adding a model involves providing the Hugging Face repo ID (if using HF format weights) and writing a small adapter if the architecture has unique aspects. Many models share the same transformer backbone structure (e.g., LLaMA variants, Qwen, etc.), so they can often reuse the existing Llama integration code with minimal changes – for example, adjusting vocabulary size or prompt format. The repository documentation was updated to guide how to extend support: they moved **setup instructions and model descriptions into a central README** (under `vllm-tt-metal-llama3/README.md`) so that all model-specific quirks are documented in one place. This means if you open that doc, you’ll find instructions on how to set up each model and what each model’s status or requirements are. The aim is to make the process of serving a new model clear: choose your model from the list, follow the steps (which might involve running `setup.sh` or pulling a container), and you’re ready to go.

In summary, TT-Inference-Server already supports a rich set of models – from 1B all the way to 70B+ – covering both language and vision, with a roadmap of more (given many are “preview”). The supported models are configured via a registry inside the code, which ties together the model weights, the code path (which kernel implementations to use), and the exact version of Tenstorrent’s libraries needed. This tight configuration control allows the server to reliably bring up even very large models (like 70B) on Tenstorrent hardware with minimal user hassle. Users just need to ensure they have the model weights (downloadable via Hugging Face) and the server takes care of the rest, such as weight loading and splitting across devices if needed (for example, a 70B model might automatically utilize multiple Tenstorrent chips in parallel if available, using tensor parallelism – TP appears in some model notes).
